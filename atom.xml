<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[SysAdmin's Journey]]></title>
  <link href="http://sysadminsjourney.com/atom.xml" rel="self"/>
  <link href="http://sysadminsjourney.com/"/>
  <updated>2012-09-18T18:32:11-05:00</updated>
  <id>http://sysadminsjourney.com/</id>
  <author>
    <name><![CDATA[Justin Ellison]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Using Git Submodules with Dynamic Puppet Environments]]></title>
    <link href="http://sysadminsjourney.com/content/using-git-submodules-dynamic-puppet-environments"/>
    <updated>2012-02-06T00:00:00-06:00</updated>
    <id>http://sysadminsjourney.com/content/using-git-submodules-with-dynamic-puppet-environments</id>
    <content type="html"><![CDATA[<p>There comes a point in the lifecycle of every Puppet setup where you realize
that you&#8217;re going to be much better off utilizing other peoples&#8217; Puppet
modules whenever possible.  It&#8217;s what makes OSS great &#8211; why should I reinvent
the wheel when I can help make your wheel even better?  I&#8217;ve found what I
think is a very productive setup &#8211; it leverages Git (specifically branches,
submodules, and hooks), Gitolite permissions, and Puppet environments to
create a workflow that a team of admins can use to iterate over new features
on without disturbing each other.</p>

<p>Most of pieces to this puzzle are very well documented elsewhere, I&#8217;ll provide
links where necessary.</p>

<h2>Step 1: Establish Dynamic Environment Workflow</h2>

<p>The first step is to go read &#8221;<a href="http://puppetlabs.com/blog/git-workflow-and-puppet-environments/">Git Workflow and Puppet
Environments</a>&#8221; written by Adrien Thebo of Puppet Labs.  Once you&#8217;ve
implemented that setup, you should be able to do the following from your
workstation:</p>

<div>
  <pre><code class='bash'>git clone git@git:puppet.git
cd puppet
git checkout -b mybrokenbranch
echo &quot;this line breaks everything&quot; &gt;&gt; manifests/site.pp
git commit -am 'Intentionally breaking things'
git push origin mybrokenbranch</code></pre>
</div>


<p>At this point, you now have a new environment named &#8216;mybrokenbranch&#8217; on your
Puppetmaster. You can test the setup by ssh&#8217;ing into the client machines and
run:</p>

<div>
  <pre><code class='bash'>puppet agent --test --environment mybrokenbranch --noop</code></pre>
</div>


<p>That obviously won&#8217;t be a happy puppet run. The key
point here being that your other environments are not impacted by the work of
this one admin. Let&#8217;s delete the local and remote branch. From your
workstation:</p>

<div>
  <pre><code class='bash'>git checkout master
git branch -d mybrokenbranch
git push origin :mybrokenbranch</code></pre>
</div>


<p>Note that
the Puppetmaster says that it&#8217;s deleted the environment. Feel free to verify
that by running the above command on the Puppet client, it will complain about
not having an environment.</p>

<h2>Step 2: Incorporate Git Submodules</h2>

<p>With all that setup, let&#8217;s go ahead and implement support for git submodules.
I have a pull request off to Adrien to implement this functionality, but until
he commits it in, you can use <a href="https://github.com/justintime/puppet-git-hooks">my fork on github</a>. Replace the update
hook with the updated version on your git server. Now, let&#8217;s try pulling a git
submodule into our repo. Again, from your workstation:</p>

<div>
  <pre><code class='bash'>git checkout -b firewall
git submodule add git://github.com/puppetlabs/puppetlabs-firewall.git modules/firewall
git add .gitmodules modules/firewall
git commit -m 'Adding firewall submodule'
git push origin firewall</code></pre>
</div>


<p>Note in the output that the Puppetmaster is checking out the
git submodule into the new environment. Go ahead and log into the
Puppetmaster, and look in your firewall environment, you should see all the
manifests and whatnot there.</p>

<p>Here&#8217;s where I need to stamp a disclosure notice &#8211; git submodules aren&#8217;t all
milk and honey. There&#8217;s some funky situations you can get yourself into if
you&#8217;re not careful. Thankfully, there&#8217;s not many of those situations you can&#8217;t
get yourself out of. I highly recommend reading the <a href="http://progit.org/book/ch6-6.html">Pro Git chapter on submodules</a> before doing anything with
them.</p>

<h2>Step 3: Implement Access Controls on Gitolite</h2>

<p>This next step is entirely optional, but works out well for us. We have a
setup where I&#8217;m the only admin that can write to the master and testing
branches of our git repo, but any sysadmin can create their own branch, test
it, and delete it if need be. <a href="http://sitaramc.github.com/gitolite/">Setting up gitolite</a> is far beyond the scope of
this post, but if you have about an hour of free time, you can have it setup
and running. However, below I&#8217;ve pasted the relevant snippet from
gitolite.conf that enforces those permissions.</p>

<div>
  <pre><code class='bash'>repo    puppet                                                                                                          
  RW+     = JustinEllison
  R       = @SysAdmins Fisheye-puppet PuppetMaster
               - master testing = @SysAdmins
  RW+     = @SysAdmins</code></pre>
</div>


<h2>Step 4: Profit!</h2>

<p>To summarize it all, here&#8217;s the workflow for an admin to add a new feature in
our Puppet setup:</p>

<ol>
<li>Create a new VM which will be the testing ground for the new feature.</li>
<li>Create a local feature branch to implement the new feature in. The admin iterates over this branch (pushing the branch to origin) getting things working with his VM.</li>
<li>Once he&#8217;s happy with the results on his VM, he&#8217;s required to login to another sandbox VM, and run it against the same puppet branch with the &#8216;&#8211;noop&#8217; flag to ensure nothing unintended happens.</li>
<li>At this point, the positive and the negative have been tested, and he then asks me to merge the feature branch into master.</li>
<li>I then do a <div>
<pre><code class='bash'>git diff ...origin/newfeature</code></pre>
</div>
We go over any changes, and I merge it in.</li>
<li>From there, we follow our normal deployment method of tagging a release, and manually checking out the tag on the Puppetmaster.</li>
</ol>


<p>While it&#8217;s certainly not perfect, this workflow setup has allowed us to work
together as a team while still implementing some best practices. In
particular, the dynamic environments allow us to test our features extensively
before releasing them into production. This is especially important in a team
where the admins aren&#8217;t Ruby programmers that can write puppet-rspec tests.</p>

<p>Before the integration of git submodules with the dynamic environment
workflow, we were manually merging external repos into our own setup, and it
was an absolute nightmare. Now, to update our repo to use a new version of
someone else&#8217;s module, we just create a new feature branch, update the
submodule, test, and merge.</p>

<p>What workflows do you and your team use that make life with Puppet better?
Please share below.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[VPS.net review]]></title>
    <link href="http://sysadminsjourney.com/content/2011/11/14/vpsnet-review"/>
    <updated>2011-11-14T00:00:00-06:00</updated>
    <id>http://sysadminsjourney.com/content/2011/11/14/vps-net-review</id>
    <content type="html"><![CDATA[<p>I&#8217;ve been running a single node from <a href="http://vps.net">VPS.net</a> for about a
year now.  Please note that my specific experience has been in their &#8220;Chicago
Zone D data center&#8221;, but if you check out their <a href="http://status.vps.net">status
page</a> or search
<a href="http://twitter.com/#!/search/%23vpsnet">Twitter</a>, you&#8217;ll find a lot of others
having the same issues.  While there&#8217;s a lot of good things to write about,
where they fail is the most important area to me: availabilty. The pros of
using VPS.net include pricing, control panel, and console level access.  As is
typical for a VPS provider, they give you many &#8220;add-on&#8221; options such as
backup, etc that you can enable &#8211; I&#8217;ve not investigated them myself.  Perhaps
the one of the nicest features is the ability to add server resources or
&#8220;nodes&#8221; on the fly with minimal downtime. However, it seems that VPS.net has
made a horrible choice in selecting what SAN they use to back their VM&#8217;s.
Examine the graphic below: <img src="http://sysadminsjourney.com/assets/images/vps-net-availabiltiy.png" alt="" /> As you
can see, I&#8217;m getting less than 2 nines worth of uptime from my node.  Each and
everytime there&#8217;s been an issue, support has been quick to point out that
they&#8217;ve had some sort of SAN issue, and that the SAN is &#8216;resyncing&#8217;.  The
problem is that while the SAN is resyncing, I/O to my node is so horrible, I
can&#8217;t cat a 500 byte file to stdout in less than 10 seconds.  So, the node
will respond to a ping, but it can&#8217;t serve up a static image via Apache.  For
all intents and purposes, that&#8217;s down in my book. The <a href="http://status.vps.net/2011/10/chi-d-cloud/">last SAN
synchronization</a> took the better
part of two days, during which time my node was unusable. In my experience,
the SAN is the most important building block when architecting a service
that&#8217;s meant to be highly available.  Until VPS.net can address their SAN
issues, they are likely to continue to have prolonged downtimes.  Until that&#8217;s
been fixed, there&#8217;s just no way I can recommend their services to anyone.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[It's not you, it's me: Call for Node Gallery co-maintainers]]></title>
    <link href="http://sysadminsjourney.com/content/2011/10/19/its-not-you-its-me-call-node-gallery-co-maintainers"/>
    <updated>2011-10-19T00:00:00-05:00</updated>
    <id>http://sysadminsjourney.com/content/2011/10/19/its-not-you-its-me-call-for-node-gallery-co-maintainers</id>
    <content type="html"><![CDATA[<p>There&#8217;s only a certain amount of bandwidth in a person&#8217;s day. As you get
older, that bandwidth seems to become more and more constrained. Kids are
extreme bandwidth hogs :) Over the years I&#8217;ve found that I have enough
bandwidth in my life to deal with one obsession that&#8217;s not part of my day job
at any given time. For the last couple years, that obsession has been with
<a href="http://drupal.org">Drupal</a> and specifically with <a href="http://drupal.org/project/node_gallery">Node
Gallery</a>. In my very biased opinion,
it&#8217;s the most user-friendly and integrated gallery experience you can have
with Drupal 6.x. Also IMHO, there&#8217;s a huge void in Drupal 7 with respect to
butt-kicking gallery modules, one that&#8217;s begging to be filled with a Node
Gallery 7.x branch. But I just can&#8217;t bring myself to that one simple git
command. I&#8217;ve had several changes at work in the past year, and I&#8217;m no longer
working with Drupal and PHP on a regular basis. I&#8217;ve become enthralled with
Puppet as of late, and that&#8217;s proven to be the gateway drug to the devops
movement for me. I&#8217;m reading books on Kanban, learning a bit of Ruby, building
deployment pipelines, and soaking up anything I can on devops. It seems
sysadmins who can code really do have a place in the world, and it appears to
be in devops. It&#8217;s not burnout, it&#8217;s simply a matter of prioritization on
demands for a limited resource. There&#8217;s just no time left over for Drupal
anymore. Back to the point of this post &#8211; Node Gallery needs a co-maintainer
who can take the module into the 7.x branch. The recently released 6.x-3.x
branch has proven to be quite stable, and would likely require only very
minimal maintenance. You can take it for a spin on the <a href="http://ng3demo.sysadminsjourney.com">demo
site</a>, or read all about it&#8217;s features on
the <a href="http://drupal.org/project/node_gallery">project page</a>. Here&#8217;s some quick
points:</p>

<ul>
<li>It has a reported user base of just under 3,800 sites, which puts it at right around #400 on the top modules list.</li>
<li>It has a great user base that&#8217;s proven to be active in the issue queue. Many of the support requests have been resolved by members of the community whom have never written a line of code. It has a strong German presence, and has been translated.</li>
<li>It integrates very tightly with <a href="http://drupal.org/project/views">Views</a>, and supports bulk uploading with <a href="http://drupal.org/project/plupload">Plupload</a>. It has it&#8217;s own access module in <a href="http://drupal.org/project/node_gallery_access">Node Gallery Access</a>, as well as a handful of other modules (all of which are listed on the <a href="http://drupal.org/project/node_gallery">project page</a>) it integrates with very well.</li>
<li>It&#8217;s been engineered to perform well from the start. If your server can handle the load of 100,000 nodes, there&#8217;s no reason it should be able to handle 100,000 Node Gallery image nodes &#8211; even if those are all in one gallery.</li>
<li>The administration UI aims to provide a working gallery setup out-of-the-box that works for 90% of the users, yet provide enough buttons and knobs for the remaining 10% to be able to tweak what they need.</li>
<li>It runs the gamut of technologies in Drupal; making use of caching, Views integration, jQuery and jQuery UI, CCK, Node Access, Batch API, etc.</li>
<li><p>What differentiates Node Gallery from most other gallery modules is that each and every image in a gallery isn&#8217;t just a field, it&#8217;s an entire node. This opens up huge possibilities for interactions with other contrib modules. The original reason for me selecting Node Gallery was because it was the only way I could sell individual images using Ubercart.
Who I&#8217;m looking for:</p></li>
<li><p>This module is likely a bit complex for someone who&#8217;s never maintained a module before. If you&#8217;ve maintained your own Drupal module (either privately or on d.o), take a look at the code and make sure you can understand what&#8217;s going on.</p></li>
<li>Drupal 7 API experience is a must; experience in migrating D6 modules to D7 is a plus.</li>
<li>Ideally, you need to have an &#8220;itch that needs scratching&#8221; &#8211; in other words, you should probably have a need for an image gallery.
If you&#8217;d like to take a crack at bringing Node Gallery to Drupal 7, <a href="http://drupal.org/user/99149/contact">contact
me</a>, or <a href="http://drupal.org/node/add/project-issue/node_gallery">file an
issue</a> in the issue
queue.</li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Drupal on Heroku]]></title>
    <link href="http://sysadminsjourney.com/content/2011/09/20/drupal-heroku"/>
    <updated>2011-09-21T00:00:00-05:00</updated>
    <id>http://sysadminsjourney.com/content/2011/09/20/drupal-on-heroku</id>
    <content type="html"><![CDATA[<p><a href="http://www.heroku.com">Heroku</a> has been around for awhile now, but has been primarily a rails host. Well, <a href="http://blog.heroku.com/archives/2011/9/15/facebook/">until recently</a> anway. With the announcement of their Facebook integration, many others have noted that *any* PHP app can at least parse on Heroku&#8217;s cedar stack. I&#8217;ll be honest, it took me longer to get ruby+rails setup on my Macbook than it did to get a proof-of-concept installation of Drupal up and running. Here&#8217;s what I did:</p>

<ol>
<li>Get ruby, rails, and the heroku gem installed and running. <a href="http://pragmaticstudio.com/blog/2010/9/23/install-rails-ruby-mac">This page</a> had me up and running pretty quickly on my Mac.</li>
<li><p>Download and extract Drupal:
<div>
<pre><code class='console'>curl http://ftp.drupal.org/files/projects/drupal-6.22.tar.gz | tar -xzvf -; cd drupal-6.22</code></pre>
</div></p></li>
<li><p>Initialize your git repo:
<div>
<pre><code class='console'>git init</code></pre>
</div></p></li>
<li><p>Here&#8217;s what makes all this proof-of-concept only. Many of the features used in Drupal core&#8217;s .htaccess file assume that the webhost has enabled the &#8220;AllowOverride All&#8221; option. Heroku doesn&#8217;t allow this, it only allows a small subset of overrides. <strong>DOING THIS WILL MORE THAN LIKELY COMPROMISE THE SECURITY OF YOUR DRUPAL INSTALL. </strong>Open up .htaccess in your editor, and comment out any line that starts with these strings:</p>

<ul>
<li>Order</li>
<li>Options</li>
<li>DirectoryIndex</li>
<li>php_value</li>
</ul>
</li>
<li><p>Add Drupal to git, and commit:
<div>
<pre><code class='console'>git add .; git commit -m 'initial commit'</code></pre>
</div></p></li>
<li><p>Create your heroku application. You&#8217;ll need to have signed up for a free account on http://www.heroku.com and give the following command your login credentials:</p>

<p><div>
<pre><code class='console'>heroku create --stack cedar</code></pre>
</div></p></li>
<li><p>Push your code up to heroku (note the URL it gives you back):
<div>
<pre><code class='console'>git push heroku master</code></pre>
</div></p></li>
<li><p>Now, we need to setup the Postgres instance:</p>

<p><div>
<pre><code class='console'>heroku addons:add shared-database</code></pre>
</div></p></li>
<li><p>Let&#8217;s display our Postgres credentials:</p>

<p><div>
<pre><code class='console'>heroku config</code></pre>
</div></p></li>
<li><p>You can now hit your Drupal instance at the URL given to you by your last git push. Install as you normally would, selecting Postgres as your database, and filling in the user, password, database, and host given to you by &#8216;heroku config&#8217;. Make sure to change the host from localhost under the &#8220;Advanced&#8221; fieldset.</p></li>
</ol>


<p>  At this point, you can poke around your install, and start seeing what all else is broken :) &#8216;heroku logs -t&#8217; is your friend. If you don&#8217;t believe me, <a href="http://electric-mountain-6735.herokuapp.com/">here&#8217;s a D7 instance</a>, and <a href="http://freezing-light-7795.herokuapp.com/">here&#8217;s a D6 one</a>.</p>

<p>  Seriously, the .htaccess point is a deal-breaker. Unless someone with more time on their hands than I do can suggest a more secure configuration (or heroku allows Drupal to override all), there&#8217;s some serious security ramifications to commenting out the lines in .htaccess.</p>

<p>  Drupal is definitely slow on the free plan for Heroku, but I mean, it&#8217;s free; what did you expect? Drupal 6 seemed to work throughout, but I noticed when getting D7 up and running that I couldn&#8217;t hit some &#8220;heavy&#8221; URL&#8217;s like /admin/configure and /admin/reports/status. I could get into other sub-menus such as /admin/configure/development/performance. We all know D7 takes a fair amount of horsepower to run, and horses aren&#8217;t free :). The whole point of heroku is being able to scale your app by dragging a slider in a web ui, and there&#8217;s no reason to believe that Drupal wouldn&#8217;t start running much faster given more resources from a non-free plan.</p>

<p>  The point of this blog post was to just jot down my notes and save someone else a little time in getting started &#8211; hopefully the community can come up with some ideas so we have another awesome choice in Drupal hosting!</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Selecting the right CDN for YOUR website]]></title>
    <link href="http://sysadminsjourney.com/content/2011/04/19/selecting-right-cdn-your-website"/>
    <updated>2011-04-19T00:00:00-05:00</updated>
    <id>http://sysadminsjourney.com/content/2011/04/19/selecting-the-right-cdn-for-your-website</id>
    <content type="html"><![CDATA[<p>At one of my jobs, we recently went through the process of selecting a
<a href="http://en.wikipedia.org/wiki/Content_delivery_network">CDN (Content Delivery Network)</a> for our site. While the first rule of
CDN’s is that “any CDN is better than no CDN”, it can be argued that
certain CDN’s are a better fit in certain situation than others. This
post is basically a summary of the process we went through when
selecting our CDN. By no means is this a statement of “XYZ is better
than ABC”, it’s simply documentation of the process we went through in
order to select the right one for our business. While most CDN’s are
compatible with Drupal via excellent contrib modules such as <a href="http://drupal.org/project/cdn">CDN</a>,
this information presented in this article is relative to any website
and isn’t Drupal-specific.</p>

<p>To illustrate the importance of a CDN using real numbers, one image
being fetched from the data center to our office takes about 323ms. That
same image fetched from Seattle is 483ms, and from Washington DC takes
599ms. The worst cases appear when coming overseas - to fetch the same
image from Paris it takes on average 1,141ms <strong>for just that one image</strong>.</p>

<p>A Content Delivery Network (CDN) shortens that distance between your
static content and the end-user. While the text on most web pages is
dynamic, most images, JavaScript, and CSS are static. These static
objects make up a large percentage of the total bytes downloaded for
each page view. By using a CDN, you place static content as close to the
end-user as possible. In turn this decreases the page load time a
end-user experiences by leaps and bounds.</p>

<h3>Pre-selection Criteria</h3>

<p>There’s a plethora of CDN’s to choose from, and if you don’t filter the
initial list down to five or fewer providers, you’ll end up spending
months in evaluation time. By defining specific must-have features, we
were able to limit the initial number of companies to compare to four.
Many CDN’s provide value-add services above and beyone static objects,
such as “Dynamic Site Acceleration” – this evaluation looked solely at
serving up static file content, e.g. JPEG, GIF, CSS, and Javascript.</p>

<p>The filtering properties we used to limit scope were:</p>

<ol>
<li><p><strong>The CDN must provide “origin pull” or “reverse proxy” support</strong>. If
the CDN receives a request for a file that doesn’t exist at the edge, it
applies a customer-defined URL rewrite to the request, and proxies the
request to the origin site. If the image exists at the origin, the edge
server caches the image locally and serves it to the client from there.
For example, the CDN host name might be cdn.example.com (which points to
the edge), and the origin site (my server) would be www.example.com. If
I point my browser to http://cdn.example.com/logo.gif, and that file
doesn’t exist at the edge, the CDN will make a request for
http://www.example.com/logo.gif. If that file exists, it is fetched and
cached. If it doesn’t exist, a 404 is returned to the client. The trade
off is that you don’t have to pre-seed static content to the CDN, but
the first user request for a static object takes a bit longer to
complete (because it results in two requests instead of one). Once the
edge network’s cache is primed, there is no performance difference
between origin pull and CDN origin.</p></li>
<li><p><strong>The CDN must propagate cache-related HTTP headers from the origin to
the end-user</strong> We’ve went to great lengths to use versioning of
filenames so that we can set far-future expires headers on 99% of our
static content as recommended by <a href="https://developer.yahoo.com/performance/rules.html">Yahoo’s “Best Practices for Speeding Up Your Website”</a>.
This results in far fewer HTTP requests to render a
page that has already been requested by the end-user previously,
ultimately decreasing page response time. Some CDN’s that offer origin
pull do not proxy these headers back.</p></li>
<li><p><strong>The CDN must use GZip compression on text-based content</strong> Most CDN’s
support this, but it’s something you definitely want to check. When
serving up static text-based content such as CSS or Javascript, the CDN
can and should compress it for you before sending it to the end-user.
Compression makes the overall page content smaller, and therefore faster
to render.</p></li>
<li><p><strong>Response time must be consistent and fast</strong> Performance is a tricky
thing. While having the fastest response time overall didn’t guarantee
that a CDN would “win”, having consistent relative poor performance
would guarantee a CDN would “lose”. Try not to focus too much on
performance numbers – most of the CDN’s will have a standard deviation
less than ten milliseconds between each other. In our research we found
out quickly that there’s a lot of features more important to us than 5ms
worth of response time.</p></li>
<li><p><strong>100% Uptime SLA</strong> Since a CDN is at it’s most basic level a
geographically distributed cluster of cache servers, it should be
implied that a CDN can provide 100% uptime. If one POP goes down,
requests should be automatically routed to the next nearest POP. If your
CDN doesn’t offer this, you need a new CDN.</p></li>
<li><p><strong>Company financial strength and solvency</strong> This is something often
overlooked when people evaluate, but was very important to us. There are
a lot of CDN’s out there, and we found only 2 or 3 that could put in
writing that they are a profitable corporation. Our implementation
required a fair amount of work, and it would take us some time to switch
to another CDN. If your CDN goes dark in the middle of the night, how
long will it take you to switch?</p></li>
</ol>


<h3>Important Features</h3>

<p>Whereas not meeting any of the above requirements would result in being
excluded from our comparison, the following features were key points of
consideration. Not meeting them all wouldn’t exclude a CDN, but on the
flip side, implementing them all would put the CDN in very good
standing.</p>

<ol>
<li><p><strong>Price</strong>. While high prices weren’t going to scare us away, bang
for the buck played a large part in our decision. We weren’t
interested in paying a premium for brand recognition.</p></li>
<li><p><strong>Strong international presence</strong>. Our guests include international
clients, and poor static object performance for those clients was
the key motivation for implementing a CDN in the first place.</p></li>
<li><p><strong>Contract terms</strong>. Some CDN’s do month-to-month, some do 12 month,
others require longer as you negotiate price.</p></li>
<li><p><strong>Overage fees</strong>. CDN’s meter you on the amount of bandwidth you
consume. You pay for a “bucket”. No CDN’s turn your service off
after you exceed that bucket, they just bill you for overages. The
good CDN’s will bill you at the same per-GB rate that you pay for
your monthly bucket. Some CDN’s charge as much as 2x for overages.
Avoid those.</p></li>
<li><p><strong>Traffic accounting</strong>. One other thing often overlooked with origin
pull CDN’s is whether or not the traffic between the edge POP’s and
the customer origin is counted as traffic against your total. Some
CDN’s count it against your bucket, others don’t.</p></li>
<li><p><strong>Setup fees</strong>. CDN’s vary wildly on their setup fees. Some are
free, some charge more than $5,000. Make sure you incorporate that
cost into your decision.</p></li>
<li><p><strong>User Interface</strong>. All CDN’s offer some form of web-based
interface. The quality of the interface greatly differs between
CDN’s. I could swear that some of the interfaces I saw were written
in CGI Perl in the late 90’s. Others interfaces offered everything a
customer could ever want, including detailed analytics and
reporting. Key questions to ask are “If I get a bad image out on the
edge, how do I purge it?”, and “How do I tell how much bandwidth is
being consumed throughout the CDN at any particular point in time?”</p></li>
</ol>


<h3>External Reporting Data</h3>

<p>We chose to invest in one month’s worth of reporting from
<a href="http://cloudharmony.com/reports">CloudHarmony’s CloudReports</a> service. This gave us a quick way to
examine performance of CDN’s to the actual end-user browser behind a
real cable/dsl/dialup connection (not to a datacenter somewhere). While
some might view those reports expensive, we found it quite a bargain to
have another independent view into the performance of a vast majority of
CDN’s.</p>

<h3>The Contenders</h3>

<p>Given the above requirements, coupled with the performance data provided
from CloudHarmony we were able to refine our list of CDN’s to consider.
In alphabetical order:</p>

<ol>
<li><a href="http://www.akamai.com">Akamai</a></li>
<li><a href="http://www.cachefly.com">CacheFly</a></li>
<li><a href="http://www.edgecast.com">EdgeCast</a></li>
<li><a href="http://www.limelight.com">LimeLight</a></li>
</ol>


<h3>First elimination: Akamai</h3>

<p>Akamai is to the CDN market what Bose is to the home audio market. While
it’s not inherently a bad product, you’re paying a huge premium for the
brand name. While we never got so far as to setup a demo account, the
performance data provided by CloudHarmony and other sources didn’t favor
them well at all. My personal opinion (which is little more than a wild
guess) on why Akamai doesn’t perform as well is because of their
product’s age. Their network is by far the largest one out there, and I
can guess that keeping up with the latest optimizations and protocols is
a huge undertaking.</p>

<p>When speaking with Akamai, I got the impression that they really don’t
care to sell their static object delivery product by itself. Their reps
focused mostly on trying to upsell their Dynamic Site Acceleration
product. While DSA might indeed be a great product, it wasn’t what we
were interested in.</p>

<p>In the end, the best price I could get out of Akamai was more than twice
that of the next most expensive CDN in our comparison, and they wanted a
3 year contract at that price. I’m just not that into paying twice as
much for an equal product, so they were eliminated. If we should move to
a Dynamic Site Acceleration type of service later, Akamai will
definitely be re-evaluated at that time.</p>

<h3>Second elimination: LimeLight</h3>

<p>LimeLight Networks is the 2nd largest CDN provider, behind Akamai. It’s
fitting that they are right behind Akamai, because they came across like
a smaller Akamai to me. Their pricing is much more competitive than
Akamai, and performance appeared to be quite good across the board. They
supposedly have a nice web and reporting interface, but I was unable to
get a demo setup without filling out paperwork that would have required
approval from our legal department. Therein lies the problem with
LimeLight – getting them to do anything outside the everyday norm was
like pulling teeth. Like Akamai, LimeLight also is focused on the upsell
and seemed to me generally disinterested in selling their static
delivery service.</p>

<p>If for some reason, we had to switch away from our primary choice, my
second choice would likely be LimeLight Networks, but only after I was
able to obtain a demo account so that I could verify their performance
was within acceptable range and the functionality of their user
interface.</p>

<h3>Independent Performance comparisons</h3>

<p>I was able to easily procure demo accounts from EdgeCast and CacheFly,
so I set up some performance testing of our own using <a href="http://www.pingdom.com">Pingdom</a> to
download a typical JPEG image from each Pingdom POP using the origin
pull setup. Note that since Pingdom’s servers are in data centers and
not in actual residences; this isn’t a measure of end-to-end
performance, rather a way to compare apples to apples response time from
various regions around the world. The executive summary here is that
while EdgeCast “edged” out CacheFly, the real message is that any CDN is
so much better than none at all:</p>

<table>
<thead>
<tr>
<th align="left">CDN                  </th>
<th align="left"> US/Non-US    </th>
<th align="left"> Location                 </th>
<th align="left"> # of Polls </th>
<th align="left"> Avg Response Time </th>
<th align="left"> Max Response Time </th>
<th align="left"> StdDev</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left">CacheFly             </td>
<td align="left"> Non-US       </td>
<td align="left"> Amsterdam 2, Netherlands </td>
<td align="left"> 289        </td>
<td align="left"> 68                </td>
<td align="left"> 4202              </td>
<td align="left"> 285.98</td>
</tr>
<tr>
<td align="left">                     </td>
<td align="left">              </td>
<td align="left"> Copenhagen, Denmark      </td>
<td align="left"> 259        </td>
<td align="left"> 158               </td>
<td align="left"> 461               </td>
<td align="left"> 36.02 </td>
</tr>
<tr>
<td align="left">                     </td>
<td align="left">              </td>
<td align="left"> Frankfurt, Germany       </td>
<td align="left"> 287        </td>
<td align="left"> 41                </td>
<td align="left"> 567               </td>
<td align="left"> 32.38 </td>
</tr>
<tr>
<td align="left">                     </td>
<td align="left">              </td>
<td align="left"> London 2, UK             </td>
<td align="left"> 290        </td>
<td align="left"> 29                </td>
<td align="left"> 2489              </td>
<td align="left"> 145.26</td>
</tr>
<tr>
<td align="left">                     </td>
<td align="left">              </td>
<td align="left"> London, UK               </td>
<td align="left"> 284        </td>
<td align="left"> 29                </td>
<td align="left"> 127               </td>
<td align="left"> 11.30 </td>
</tr>
<tr>
<td align="left">                     </td>
<td align="left">              </td>
<td align="left"> Madrid, Spain            </td>
<td align="left"> 259        </td>
<td align="left"> 201               </td>
<td align="left"> 586               </td>
<td align="left"> 31.36 </td>
</tr>
<tr>
<td align="left">                     </td>
<td align="left">              </td>
<td align="left"> Manchester, UK           </td>
<td align="left"> 281        </td>
<td align="left"> 129               </td>
<td align="left"> 1709              </td>
<td align="left"> 184.87</td>
</tr>
<tr>
<td align="left">                     </td>
<td align="left">              </td>
<td align="left"> Montreal, Canada         </td>
<td align="left"> 286        </td>
<td align="left"> 105               </td>
<td align="left"> 3084              </td>
<td align="left"> 250.63</td>
</tr>
<tr>
<td align="left">                     </td>
<td align="left">              </td>
<td align="left"> Paris, France            </td>
<td align="left"> 286        </td>
<td align="left"> 143               </td>
<td align="left"> 521               </td>
<td align="left"> 60.11 </td>
</tr>
<tr>
<td align="left">                     </td>
<td align="left">              </td>
<td align="left"> Stockholm, Sweden        </td>
<td align="left"> 286        </td>
<td align="left"> 54                </td>
<td align="left"> 882               </td>
<td align="left"> 80.88 </td>
</tr>
<tr>
<td align="left">                     </td>
<td align="left"> Non-US Total </td>
<td align="left">                          </td>
<td align="left"> 2807       </td>
<td align="left"> 94                </td>
<td align="left"> 4202              </td>
<td align="left"> 157.88           </td>
</tr>
<tr>
<td align="left">                     </td>
<td align="left"> US           </td>
<td align="left"> Atlanta, Georgia         </td>
<td align="left"> 289        </td>
<td align="left"> 16                </td>
<td align="left"> 398               </td>
<td align="left"> 23.52 </td>
</tr>
<tr>
<td align="left">                     </td>
<td align="left">              </td>
<td align="left"> Chicago, IL              </td>
<td align="left"> 288        </td>
<td align="left"> 56                </td>
<td align="left"> 2615              </td>
<td align="left"> 158.33</td>
</tr>
<tr>
<td align="left">                     </td>
<td align="left">              </td>
<td align="left"> Dallas 4, TX             </td>
<td align="left"> 286        </td>
<td align="left"> 40                </td>
<td align="left"> 960               </td>
<td align="left"> 74.61 </td>
</tr>
<tr>
<td align="left">                     </td>
<td align="left">              </td>
<td align="left"> Dallas 5, TX             </td>
<td align="left"> 289        </td>
<td align="left"> 26                </td>
<td align="left"> 1506              </td>
<td align="left"> 89.08 </td>
</tr>
<tr>
<td align="left">                     </td>
<td align="left">              </td>
<td align="left"> Dallas 6, TX             </td>
<td align="left"> 291        </td>
<td align="left"> 47                </td>
<td align="left"> 1473              </td>
<td align="left"> 132.25</td>
</tr>
<tr>
<td align="left">                     </td>
<td align="left">              </td>
<td align="left"> Denver, CO               </td>
<td align="left"> 289        </td>
<td align="left"> 216               </td>
<td align="left"> 925               </td>
<td align="left"> 72.18 </td>
</tr>
<tr>
<td align="left">                     </td>
<td align="left">              </td>
<td align="left"> Herndon, VA              </td>
<td align="left"> 288        </td>
<td align="left"> 473               </td>
<td align="left"> 3472              </td>
<td align="left"> 196.13</td>
</tr>
<tr>
<td align="left">                     </td>
<td align="left">              </td>
<td align="left"> Houston 3, TX            </td>
<td align="left"> 289        </td>
<td align="left"> 107               </td>
<td align="left"> 382               </td>
<td align="left"> 18.15 </td>
</tr>
<tr>
<td align="left">                     </td>
<td align="left">              </td>
<td align="left"> Las Vegas, NV            </td>
<td align="left"> 288        </td>
<td align="left"> 74                </td>
<td align="left"> 3044              </td>
<td align="left"> 180.60</td>
</tr>
<tr>
<td align="left">                     </td>
<td align="left">              </td>
<td align="left"> Los Angeles, CA          </td>
<td align="left"> 289        </td>
<td align="left"> 12                </td>
<td align="left"> 92                </td>
<td align="left"> 11.52 </td>
</tr>
<tr>
<td align="left">                     </td>
<td align="left">              </td>
<td align="left"> New York, NY             </td>
<td align="left"> 289        </td>
<td align="left"> 175               </td>
<td align="left"> 2571              </td>
<td align="left"> 152.29</td>
</tr>
<tr>
<td align="left">                     </td>
<td align="left">              </td>
<td align="left"> San Francisco, CA        </td>
<td align="left"> 287        </td>
<td align="left"> 28                </td>
<td align="left"> 231               </td>
<td align="left"> 24.17 </td>
</tr>
<tr>
<td align="left">                     </td>
<td align="left">              </td>
<td align="left"> Seattle, WA              </td>
<td align="left"> 288        </td>
<td align="left"> 174               </td>
<td align="left"> 1083              </td>
<td align="left"> 108.41</td>
</tr>
<tr>
<td align="left">                     </td>
<td align="left">              </td>
<td align="left"> Tampa, Florida           </td>
<td align="left"> 267        </td>
<td align="left"> 68                </td>
<td align="left"> 3048              </td>
<td align="left"> 214.49</td>
</tr>
<tr>
<td align="left">                     </td>
<td align="left">              </td>
<td align="left"> Washington, DC           </td>
<td align="left"> 286        </td>
<td align="left"> 163               </td>
<td align="left"> 1547              </td>
<td align="left"> 141.67</td>
</tr>
<tr>
<td align="left">                     </td>
<td align="left"> US Total     </td>
<td align="left">                          </td>
<td align="left"> 4303       </td>
<td align="left"> 112               </td>
<td align="left"> 3472              </td>
<td align="left"> 170.11</td>
</tr>
<tr>
<td align="left">CacheFly Total       </td>
<td align="left">              </td>
<td align="left">                          </td>
<td align="left"> 7110       </td>
<td align="left"> 105               </td>
<td align="left"> 4202              </td>
<td align="left"> 165.61</td>
</tr>
<tr>
<td align="left">                     </td>
<td align="left">              </td>
<td align="left">                          </td>
<td align="left">            </td>
<td align="left">                   </td>
<td align="left">                   </td>
<td align="left">       </td>
</tr>
<tr>
<td align="left">EdgeCast Small       </td>
<td align="left"> Non-US       </td>
<td align="left"> Amsterdam 2, Netherlands </td>
<td align="left"> 284        </td>
<td align="left"> 62                </td>
<td align="left"> 381               </td>
<td align="left"> 27.49 </td>
</tr>
<tr>
<td align="left">                     </td>
<td align="left">              </td>
<td align="left"> Copenhagen, Denmark      </td>
<td align="left"> 254        </td>
<td align="left"> 126               </td>
<td align="left"> 1148              </td>
<td align="left"> 87.72 </td>
</tr>
<tr>
<td align="left">                     </td>
<td align="left">              </td>
<td align="left"> Frankfurt, Germany       </td>
<td align="left"> 284        </td>
<td align="left"> 40                </td>
<td align="left"> 318               </td>
<td align="left"> 19.05 </td>
</tr>
<tr>
<td align="left">                     </td>
<td align="left">              </td>
<td align="left"> London 2, UK             </td>
<td align="left"> 284        </td>
<td align="left"> 26                </td>
<td align="left"> 975               </td>
<td align="left"> 59.59 </td>
</tr>
<tr>
<td align="left">                     </td>
<td align="left">              </td>
<td align="left"> London, UK               </td>
<td align="left"> 283        </td>
<td align="left"> 23                </td>
<td align="left"> 191               </td>
<td align="left"> 14.38 </td>
</tr>
<tr>
<td align="left">                     </td>
<td align="left">              </td>
<td align="left"> Madrid, Spain            </td>
<td align="left"> 252        </td>
<td align="left"> 176               </td>
<td align="left"> 1174              </td>
<td align="left"> 112.31</td>
</tr>
<tr>
<td align="left">                     </td>
<td align="left">              </td>
<td align="left"> Manchester, UK           </td>
<td align="left"> 275        </td>
<td align="left"> 86                </td>
<td align="left"> 1494              </td>
<td align="left"> 118.26</td>
</tr>
<tr>
<td align="left">                     </td>
<td align="left">              </td>
<td align="left"> Montreal, Canada         </td>
<td align="left"> 283        </td>
<td align="left"> 163               </td>
<td align="left"> 601               </td>
<td align="left"> 59.56 </td>
</tr>
<tr>
<td align="left">                     </td>
<td align="left">              </td>
<td align="left"> Paris, France            </td>
<td align="left"> 283        </td>
<td align="left"> 94                </td>
<td align="left"> 1537              </td>
<td align="left"> 140.76</td>
</tr>
<tr>
<td align="left">                     </td>
<td align="left">              </td>
<td align="left"> Stockholm, Sweden        </td>
<td align="left"> 271        </td>
<td align="left"> 162               </td>
<td align="left"> 967               </td>
<td align="left"> 81.87 </td>
</tr>
<tr>
<td align="left">                     </td>
<td align="left"> Non-US Total </td>
<td align="left">                          </td>
<td align="left"> 2753       </td>
<td align="left"> 94                </td>
<td align="left"> 1537              </td>
<td align="left"> 99.35            </td>
</tr>
<tr>
<td align="left">                     </td>
<td align="left"> US           </td>
<td align="left"> Atlanta, Georgia         </td>
<td align="left"> 284        </td>
<td align="left"> 129               </td>
<td align="left"> 523               </td>
<td align="left"> 34.51</td>
</tr>
<tr>
<td align="left">                     </td>
<td align="left">              </td>
<td align="left"> Chicago, IL              </td>
<td align="left"> 284        </td>
<td align="left"> 26                </td>
<td align="left"> 463               </td>
<td align="left"> 35.86 </td>
</tr>
<tr>
<td align="left">                     </td>
<td align="left">              </td>
<td align="left"> Dallas 4, TX             </td>
<td align="left"> 277        </td>
<td align="left"> 30                </td>
<td align="left"> 339               </td>
<td align="left"> 25.79 </td>
</tr>
<tr>
<td align="left">                     </td>
<td align="left">              </td>
<td align="left"> Dallas 5, TX             </td>
<td align="left"> 284        </td>
<td align="left"> 26                </td>
<td align="left"> 581               </td>
<td align="left"> 50.32 </td>
</tr>
<tr>
<td align="left">                     </td>
<td align="left">              </td>
<td align="left"> Dallas 6, TX             </td>
<td align="left"> 284        </td>
<td align="left"> 23                </td>
<td align="left"> 430               </td>
<td align="left"> 33.68 </td>
</tr>
<tr>
<td align="left">                     </td>
<td align="left">              </td>
<td align="left"> Denver, CO               </td>
<td align="left"> 281        </td>
<td align="left"> 244               </td>
<td align="left"> 2169              </td>
<td align="left"> 150.12</td>
</tr>
<tr>
<td align="left">                     </td>
<td align="left">              </td>
<td align="left"> Herndon, VA              </td>
<td align="left"> 280        </td>
<td align="left"> 24                </td>
<td align="left"> 301               </td>
<td align="left"> 20.44 </td>
</tr>
<tr>
<td align="left">                     </td>
<td align="left">              </td>
<td align="left"> Houston 3, TX            </td>
<td align="left"> 281        </td>
<td align="left"> 115               </td>
<td align="left"> 441               </td>
<td align="left"> 40.02 </td>
</tr>
<tr>
<td align="left">                     </td>
<td align="left">              </td>
<td align="left"> Las Vegas, NV            </td>
<td align="left"> 281        </td>
<td align="left"> 56                </td>
<td align="left"> 559               </td>
<td align="left"> 34.32 </td>
</tr>
<tr>
<td align="left">                     </td>
<td align="left">              </td>
<td align="left"> Los Angeles, CA          </td>
<td align="left"> 283        </td>
<td align="left"> 11                </td>
<td align="left"> 94                </td>
<td align="left"> 8.45  </td>
</tr>
<tr>
<td align="left">                     </td>
<td align="left">              </td>
<td align="left"> New York, NY             </td>
<td align="left"> 284        </td>
<td align="left"> 72                </td>
<td align="left"> 1134              </td>
<td align="left"> 161.16</td>
</tr>
<tr>
<td align="left">                     </td>
<td align="left">              </td>
<td align="left"> San Francisco, CA        </td>
<td align="left"> 280        </td>
<td align="left"> 23                </td>
<td align="left"> 118               </td>
<td align="left"> 11.01 </td>
</tr>
<tr>
<td align="left">                     </td>
<td align="left">              </td>
<td align="left"> Seattle, WA              </td>
<td align="left"> 282        </td>
<td align="left"> 131               </td>
<td align="left"> 3571              </td>
<td align="left"> 333.38</td>
</tr>
<tr>
<td align="left">                     </td>
<td align="left">              </td>
<td align="left"> Tampa, Florida           </td>
<td align="left"> 260        </td>
<td align="left"> 166               </td>
<td align="left"> 4977              </td>
<td align="left"> 303.29</td>
</tr>
<tr>
<td align="left">                     </td>
<td align="left">              </td>
<td align="left"> Washington, DC           </td>
<td align="left"> 282        </td>
<td align="left"> 83                </td>
<td align="left"> 686               </td>
<td align="left"> 111.97</td>
</tr>
<tr>
<td align="left">                     </td>
<td align="left"> US Total     </td>
<td align="left">                          </td>
<td align="left"> 4207       </td>
<td align="left"> 77                </td>
<td align="left"> 4977              </td>
<td align="left"> 148.63</td>
</tr>
<tr>
<td align="left">EdgeCast Small Total </td>
<td align="left">              </td>
<td align="left">                          </td>
<td align="left"> 6960       </td>
<td align="left"> 84                </td>
<td align="left"> 4977              </td>
<td align="left"> 131.64</td>
</tr>
<tr>
<td align="left">                     </td>
<td align="left">              </td>
<td align="left">                          </td>
<td align="left">            </td>
<td align="left">                   </td>
<td align="left">                   </td>
<td align="left">       </td>
</tr>
<tr>
<td align="left">Data Center          </td>
<td align="left"> Non-US       </td>
<td align="left"> Amsterdam 2, Netherlands </td>
<td align="left"> 292        </td>
<td align="left"> 837               </td>
<td align="left"> 1344              </td>
<td align="left"> 35.72 </td>
</tr>
<tr>
<td align="left">                     </td>
<td align="left">              </td>
<td align="left"> Copenhagen, Denmark      </td>
<td align="left"> 262        </td>
<td align="left"> 990               </td>
<td align="left"> 4195              </td>
<td align="left"> 297.90</td>
</tr>
<tr>
<td align="left">                     </td>
<td align="left">              </td>
<td align="left"> Frankfurt, Germany       </td>
<td align="left"> 291        </td>
<td align="left"> 867               </td>
<td align="left"> 1533              </td>
<td align="left"> 57.14 </td>
</tr>
<tr>
<td align="left">                     </td>
<td align="left">              </td>
<td align="left"> London 2, UK             </td>
<td align="left"> 291        </td>
<td align="left"> 725               </td>
<td align="left"> 1065              </td>
<td align="left"> 25.95 </td>
</tr>
<tr>
<td align="left">                     </td>
<td align="left">              </td>
<td align="left"> London, UK               </td>
<td align="left"> 290        </td>
<td align="left"> 811               </td>
<td align="left"> 1114              </td>
<td align="left"> 49.50 </td>
</tr>
<tr>
<td align="left">                     </td>
<td align="left">              </td>
<td align="left"> Madrid, Spain            </td>
<td align="left"> 262        </td>
<td align="left"> 1005              </td>
<td align="left"> 1765              </td>
<td align="left"> 75.84 </td>
</tr>
<tr>
<td align="left">                     </td>
<td align="left">              </td>
<td align="left"> Manchester, UK           </td>
<td align="left"> 281        </td>
<td align="left"> 899               </td>
<td align="left"> 8928              </td>
<td align="left"> 580.11</td>
</tr>
<tr>
<td align="left">                     </td>
<td align="left">              </td>
<td align="left"> Montreal, Canada         </td>
<td align="left"> 291        </td>
<td align="left"> 342               </td>
<td align="left"> 412               </td>
<td align="left"> 11.52 </td>
</tr>
<tr>
<td align="left">                     </td>
<td align="left">              </td>
<td align="left"> Paris, France            </td>
<td align="left"> 293        </td>
<td align="left"> 1128              </td>
<td align="left"> 2680              </td>
<td align="left"> 230.78</td>
</tr>
<tr>
<td align="left">                     </td>
<td align="left">              </td>
<td align="left"> Stockholm, Sweden        </td>
<td align="left"> 292        </td>
<td align="left"> 1063              </td>
<td align="left"> 4056              </td>
<td align="left"> 367.89</td>
</tr>
<tr>
<td align="left">                     </td>
<td align="left"> Non-US Total </td>
<td align="left">                          </td>
<td align="left"> 2845       </td>
<td align="left"> 864               </td>
<td align="left"> 8928              </td>
<td align="left"> 326.71           </td>
</tr>
<tr>
<td align="left">                     </td>
<td align="left"> US           </td>
<td align="left"> Atlanta, Georgia         </td>
<td align="left"> 291        </td>
<td align="left"> 316               </td>
<td align="left"> 1017              </td>
<td align="left"> 63.48 </td>
</tr>
<tr>
<td align="left">                     </td>
<td align="left">              </td>
<td align="left"> Chicago, IL              </td>
<td align="left"> 290        </td>
<td align="left"> 170               </td>
<td align="left"> 253               </td>
<td align="left"> 7.02  </td>
</tr>
<tr>
<td align="left">                     </td>
<td align="left">              </td>
<td align="left"> Dallas 4, TX             </td>
<td align="left"> 292        </td>
<td align="left"> 191               </td>
<td align="left"> 3214              </td>
<td align="left"> 253.67</td>
</tr>
<tr>
<td align="left">                     </td>
<td align="left">              </td>
<td align="left"> Dallas 5, TX             </td>
<td align="left"> 292        </td>
<td align="left"> 145               </td>
<td align="left"> 263               </td>
<td align="left"> 14.52 </td>
</tr>
<tr>
<td align="left">                     </td>
<td align="left">              </td>
<td align="left"> Dallas 6, TX             </td>
<td align="left"> 291        </td>
<td align="left"> 147               </td>
<td align="left"> 358               </td>
<td align="left"> 22.93 </td>
</tr>
<tr>
<td align="left">                     </td>
<td align="left">              </td>
<td align="left"> Denver, CO               </td>
<td align="left"> 291        </td>
<td align="left"> 71                </td>
<td align="left"> 272               </td>
<td align="left"> 14.63 </td>
</tr>
<tr>
<td align="left">                     </td>
<td align="left">              </td>
<td align="left"> Herndon, VA              </td>
<td align="left"> 293        </td>
<td align="left"> 316               </td>
<td align="left"> 487               </td>
<td align="left"> 15.43 </td>
</tr>
<tr>
<td align="left">                     </td>
<td align="left">              </td>
<td align="left"> Houston 3, TX            </td>
<td align="left"> 293        </td>
<td align="left"> 177               </td>
<td align="left"> 372               </td>
<td align="left"> 19.66 </td>
</tr>
<tr>
<td align="left">                     </td>
<td align="left">              </td>
<td align="left"> Las Vegas, NV            </td>
<td align="left"> 290        </td>
<td align="left"> 246               </td>
<td align="left"> 3194              </td>
<td align="left"> 392.02</td>
</tr>
<tr>
<td align="left">                     </td>
<td align="left">              </td>
<td align="left"> Los Angeles, CA          </td>
<td align="left"> 291        </td>
<td align="left"> 303               </td>
<td align="left"> 1188              </td>
<td align="left"> 57.60 </td>
</tr>
<tr>
<td align="left">                     </td>
<td align="left">              </td>
<td align="left"> New York, NY             </td>
<td align="left"> 290        </td>
<td align="left"> 346               </td>
<td align="left"> 1120              </td>
<td align="left"> 123.55</td>
</tr>
<tr>
<td align="left">                     </td>
<td align="left">              </td>
<td align="left"> San Francisco, CA        </td>
<td align="left"> 293        </td>
<td align="left"> 229               </td>
<td align="left"> 519               </td>
<td align="left"> 22.28 </td>
</tr>
<tr>
<td align="left">                     </td>
<td align="left">              </td>
<td align="left"> Seattle, WA              </td>
<td align="left"> 290        </td>
<td align="left"> 489               </td>
<td align="left"> 1078              </td>
<td align="left"> 170.33</td>
</tr>
<tr>
<td align="left">                     </td>
<td align="left">              </td>
<td align="left"> Tampa, Florida           </td>
<td align="left"> 270        </td>
<td align="left"> 331               </td>
<td align="left"> 4105              </td>
<td align="left"> 247.99</td>
</tr>
<tr>
<td align="left">                     </td>
<td align="left">              </td>
<td align="left"> Washington, DC           </td>
<td align="left"> 290        </td>
<td align="left"> 595               </td>
<td align="left"> 1511              </td>
<td align="left"> 235.84</td>
</tr>
<tr>
<td align="left">                     </td>
<td align="left"> US Total     </td>
<td align="left">                          </td>
<td align="left"> 4347       </td>
<td align="left"> 271               </td>
<td align="left"> 4105              </td>
<td align="left"> 208.20</td>
</tr>
<tr>
<td align="left">Data Center Total    </td>
<td align="left">              </td>
<td align="left">                          </td>
<td align="left"> 7192       </td>
<td align="left"> 506               </td>
<td align="left"> 8928              </td>
<td align="left"> 390.52</td>
</tr>
</tbody>
</table>


<p>&#8230; and to really drive the point home for the PHB&#8217;s, we consolidate the
data and give a very telling graph:</p>

<p><img src="http://sysadminsjourney.com/assets/images/cdn.png" alt="" /></p>

<h3>Third elimination: CacheFly</h3>

<p>CacheFly is an up-and-comer in the CDN arena. They have <strong>very</strong>
aggressive pricing, and have very good performance as well. If the site
in question was a popular blog or community website and was very price
sensitive, I would select CacheFly as my first choice CDN. However,
where they fall short is in reporting and their web interface. The best
way to contact their support department is via email or web-based form.
Their web interface left a huge amount to be desired, and they have very
little documentation on how to use it. There is no reporting whatsoever
– you get raw log files and have to write our own reporting scripts on
top of that data. I couldn&#8217;t help but wonder about all the “what ifs”.
What if we get an incorrect image cached and need to have it cleared
from their network? If we see a DDoS at the CDN, how do we know? These
and other similiar questions are what ultimately eliminated CacheFly.</p>

<p>In CacheFly&#8217;s defense, I was told that they were working on a complete
refactor of the user interface and was offered a chance to help beta it,
but I was under time constraints and declined. The issues I had with the
UI may or may not be present at the time of this writing.</p>

<h3>The winner (for us): EdgeCast</h3>

<p>It will appear when reading this post that I used the process of
elimination to find the &#8220;lesser of all evils&#8221;, but understand that&#8217;s
just the writing style I chose to convey the process. It wasn&#8217;t that
EdgeCast didn&#8217;t lose, it&#8217;s that they won. Here&#8217;s why:</p>

<ul>
<li>EdgeCast is routinely in the top tier of CDN’s in terms of
performance.</li>
<li>Their support is very knowledgeable and responsive.</li>
<li>The sales reps care about your business and are willing to work with
you.</li>
<li>They offer the most features of any CDN I evaluated. One such
feature is &#8220;rollover&#8221; where if you don&#8217;t use all your allotted
transfer for one month, the remainder gets added to next months
allotment. This is perfect for a business with holiday traffic
spikes such as ours.</li>
<li>While they aren’t the cheapest CDN, they are certainly affordable,
and offered the best “bang for the buck” for the feature set we
needed.</li>
<li>Their UI is fully functional, offering configuration, reporting, and
analytics in an easy to use fashion. The UI includes a fully
functional rules engine (for additional charge) that allows you to
apply actions such as cache purge, header change, etc based upon
conditions like client IP, HTTP request header, etc.</li>
<li>Last but certainly not least, the company is one of only two
profitable CDN’s in the market today.</li>
</ul>


<h3>IT&#8217;S NOT THE DESTINATION, IT&#8217;S THE JOURNEY!!!</h3>

<p>Please don&#8217;t read this article and walk away saying &#8220;Justin recommends
EdgeCast, that&#8217;s who we&#8217;re going with&#8221;. For one, if you&#8217;re letting my
blog posts make business decisions for you instead of doing due
diligence, then you&#8217;re doing it wrong.</p>

<p>For our <strong>very specific needs</strong> EdgeCast was the best fit. For your
needs, you will very possibly arrive at a completely different decision,
and that&#8217;s great. By all means, blog about it. What I&#8217;m trying to convey
is that there are a lot of points of comparison when going through your
evaluation, and not all of them are obvious. It&#8217;s hard to get an
objective point of view when doing this on your own &#8211; this is my best
attempt at documenting what I came across.</p>

<p>Hopefully if you haven&#8217;t implemented a CDN for your busy sites, this
post will motivate you to do so. If you&#8217;re unhappy with your current
CDN, perhaps this post has given you some insight on how to find a
replacement. If you&#8217;re happy with your current CDN, please leave
comments as to why.</p>

<p>Lastly, I was in no way influenced monetarily or otherwise by any
vendors, and none of the links in this article contain referral ID&#8217;s.
This is all my personal opinion and in no way represents the opinion of
my employers.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Lead SysAdmin position available]]></title>
    <link href="http://sysadminsjourney.com/content/2011/04/06/lead-sysadmin-position-available"/>
    <updated>2011-04-06T00:00:00-05:00</updated>
    <id>http://sysadminsjourney.com/content/2011/04/06/lead-sysadmin-position-available</id>
    <content type="html"><![CDATA[<p>There&#8217;s a blog post to follow with when/why, etc., but without
further ado: I&#8217;m moving to a new position at Buckle, and that means
we need a new Lead SysAdmin.  It&#8217;s a great job at a great company,
in a great place to raise a family (Kearney, NE).  You get paid
well, get a good yearly budget for new toys, and equipment, and
it&#8217;s overall a very fun position. If interested, <a href="http://sysadminsjourney.com/contact">drop me a
line</a>, and I&#8217;ll make sure your
resume gets the proper attention.  To apply online, <a href="https://storefront.kenexa.com/buckle/cc/CCJobSearchAction.ss?command=CCSearchPage">click
here</a>,
and search for jobs within 5 miles of zip code 68845.  The job
title is &#8220;Web Development - Lead Systems Administrator&#8221;. Here&#8217;s the
job posting: JOB DETAIL Job Title:  ** Web Development - Lead
Systems Administrator**
<img src="https://storefront.kenexa.com/buckle/cc/images/orange_line.gif" alt="image" />
Location:  Buckle Corporate Office &amp; Distribution Center  2407 W
24th Street  KEARNEY, Nebraska 68845-0000
<img src="https://storefront.kenexa.com/buckle/cc/images/orange_line.gif" alt="image" />
Job Description:  ### Lead Systems Administrator **Position
Summary:** The Lead Systems Administrator will be responsible for
the deployment and maintenance of Unix/Linux systems and
application software in multiple environments. The ideal candidate
will possess a deep understanding of large scale Unix deployments
and will lead the team responsible for the infrastructure serving
all e-commerce and intranet applications. Additionally, this person
must be able to function effectively in a fast-paced environment
where projects range from maintenance to upgrades to new
deployments and technologies. Our Systems Administrators also serve
as Network Administrators for the smaller networks their systems
reside in, so strong knowledge of ethernet, TCP/IP, and network
security is required. **Responsibilities:** • Maintain all
servers and workstations on WSD team, including production,
development, and staging of servers for the e-commerce platform and
company intranet • Setup, maintain, and manage an enterprise-class
backup strategy for WSD team servers and workstations • Automate
tasks via custom scripting • Assist in architecting and designing
solid server solutions **Requirements:** • Expertise in setting
up robust and reliable server architectures. Additionally, a large
appetite for automating the mundane is preferred and will be
encouraged. ● In-depth knowledge of technologies that include but
are not limited to: Linux systems administration, Java VM tuning,
Weblogic Administration, Apache HTTPD/NGINX Administration, All
layers of TCP/IP, subnetting, etc., IPSEC VPN’s, ISC BIND, Load
balancing and clustering technologies, Shell scripting, Nagios
monitoring and RRD administration, RPM packaging format and
patching best practices • A bachelor’s degree in Computer Science
or other discipline • Minimum 4-5 years of previous
system-administration experience in a professional environment
**Compensation:** Market/negotiable, relocation assistance is a
possibility for the right candidate.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Case Study in Migrating XML to Drupal using Migrate]]></title>
    <link href="http://sysadminsjourney.com/content/2010/07/14/case-study-migrating-xml-drupal-using-migrate"/>
    <updated>2010-07-14T00:00:00-05:00</updated>
    <id>http://sysadminsjourney.com/content/2010/07/14/case-study-in-migrating-xml-to-drupal-using-migrate</id>
    <content type="html"><![CDATA[<p>Sorry for the lack of posts as of late &#8211; a massive upgrade operation at
$DAYJOB has had me out of commission for a few weeks. Also, I&#8217;ve had the great
fortune to be able to be part of a migration to Drupal which exposed me to
<a href="http://drupal.org/project/migrate">migrate</a> and friends. Yes, I said &#8220;great
fortune&#8221; in the same sentence with &#8220;migration&#8221; without using a negative -
that&#8217;s just how awesome this module is. My first impression when looking at
the documentation for migrate was that it didn&#8217;t seem complete. While it&#8217;s
true that the documentation could be better (what module couldn&#8217;t use better
documentation?), the problem is that no two migrations are alike. Because of
this, the best documentation is not going to be written by the module authors,
it will be written by the module <em>users</em> - they are the ones that come up with
the recipies to fill the cookbook. There are several good reasons why there
aren&#8217;t many recipes available:</p>

<ul>
<li>Developers don&#8217;t like doing migrations. It can be painful, and often takes quite a bit of time.</li>
<li>Users don&#8217;t like migrations. They see a migration of data as something easily done, and they often get sticker shock when presented with estimates for a large migration.</li>
<li>Migration code is written in a flurry before the site is active. Right before launch, development crescendos, and then is often never used again (because no two migrations are the same).
This being my first migration, I vowed that I would document my experience,
because I learned so much from it. In this particular migration, we had to
migrate a huge XML file into about 2,200 nodes in 3 content types. Read on for
my contribution to the cookbook! First, some discussion on the general
workflow and some design decisions. Since I had to get XML into the database
before I could run the migrate, I wrote a command line script to do just that.
When you need to manipulate data between your source and destination (i.e.,
change all references to www.olddomain.com to www.newdomain.com), you usually
have to do this via the hooks that the migrate module provides. In my case,
there were a few cases where doing the data munging in the command line script
was much easier than doing it within the hooks. The problem with making
transformations within the command line script is that with every change, I
had to re-run the script. This wasn&#8217;t a big deal, as the XML to MySQL script
took around 15 seconds to complete. I also quickly discovered that if you have
less than 10 entities of one type (Story content type, user, etc), it&#8217;s
usually better to just hand-migrate them. The most straight-forward migration
will take 1 hour at a minimum to setup and test &#8211; if it will take less than
that to copy/paste, save your time and do it the less sexy way. Since we had
to transform the XML into MySQL tables, and there was a lot of data in the XML
that we didn&#8217;t need, I decided the best way to dynamically change what we
import and what we didn&#8217;t was by using hook_install() and Drupal&#8217;s DB schema
API. By naming the MySQL table columns the same as the XML attributes, we can
add and remove data to be transformed quite easily. Lastly, I need to re-
iterate that this was my first migration. What I describe here works for me,
but may very well not be the best way to do it. Also, I will not duplicate
what you can learn from the migrate module documentation, so make sure to read
that first. Let me know any suggestions you may have in the comments.</li>
</ul>


<h2>Install Module Dependencies</h2>

<p>The first step is to install module dependencies. You&#8217;ll need
<a href="http://drupal.org/project/views">Views</a>,
<a href="http://drupal.org/project/schema">Schema</a>, <a href="http://drupal.org/project/tw">Table Wizard(tw)</a>. You&#8217;ll also want to install
<a href="http://drupal.org/project/migrate">Migrate</a>, and <a href="http://drupal.org/project/migrate_extras">Migrate Extras</a> if you want to do any work
with CCK fields. I must admit that I hadn&#8217;t seen Table Wizard before this
project, but it will always be present in my dev installs from here out. If
you find yourself using SQLYog, PHPMyAdmin, or some other tool to simply look
at data in the database, be sure to check it out.</p>

<h2>Create Our Custom Module</h2>

<p>As I mentioned above, we are relying on the Drupal Schema API to make a lot of
this easy, so let&#8217;s make a custom module that sets up our schemas for us.
We&#8217;ll call this module my_import. Create a new directory in your modules
directory, and name it my_import. First, create my_import.info with this
inside:</p>

<div>
  <pre><code class='ini'>name = My Import
description = &quot;An import module.&quot;
core = 6.x
dependencies[] = migrate
dependencies[] = migrate_extras
dependencies[] = content
dependencies[] = path_redirect
package = Database</code></pre>
</div>


<p>Nothing too wild here, just requiring some modules that we&#8217;ll
be using later. Now, create my_import.install in the same directory with this
inside:</p>

<div>
  <pre><code class='php'>&lt;?php
function my_import_schema() {
  $schema = array();
  
  $schema['clickability_articles'] = array(
    'fields' =&gt; array(
      'id' =&gt; array(
        'type' =&gt; 'int',
        'not null' =&gt; TRUE,
        'description' =&gt; t('Clickability article ID'),
      ),
      'createDate' =&gt; array(
        'type' =&gt; 'datetime',
        'not null' =&gt; TRUE,
        'description' =&gt; t('Clickability article creation date.'),
      ),
      'editDate' =&gt; array(
        'type' =&gt; 'datetime',
        'not null' =&gt; TRUE,
        'description' =&gt; t('Clickability article edit date'),
      ),
      'title' =&gt; array(
        'type' =&gt; 'text',
        'not null' =&gt; TRUE,
        'description' =&gt; t('Clickability article title'),
      ),
      'author' =&gt; array(
        'type' =&gt; 'text',
        'not null' =&gt; FALSE,
        'description' =&gt; t('Clickability content author (optional)'),
      ),
      'articleauthor' =&gt; array(
        'type' =&gt; 'text',
        'not null' =&gt; TRUE,
        'description' =&gt; t('Clickability article author'),
      ),
      'summary' =&gt; array(
        'type' =&gt; 'text',
        'not null' =&gt; TRUE,
        'description' =&gt; t('Clickability article short summary'),
      ),
      'body' =&gt; array(
        'type' =&gt; 'text',
        'not null' =&gt; TRUE,
        'size' =&gt; 'big',
        'description' =&gt; t('Clickability article body'),
      ),
      'placement' =&gt; array(
        'type' =&gt; 'text',
        'not null' =&gt; FALSE,
        'description' =&gt; t('Clickability article related article placement lists'),
      ),
      'thumbnail' =&gt; array(
        'type' =&gt; 'text',
        'not null' =&gt; FALSE,
        'description' =&gt; t('Clickability article thumbnail'),
      ),
      'image' =&gt; array(
        'type' =&gt; 'text',
        // @todo: Some articles do not have an image, but we require Master Image to be set.
        'not null' =&gt; FALSE,
        'description' =&gt; t('Clickability article image'),
      ),
      'image2' =&gt; array(
        'type' =&gt; 'text',
        'not null' =&gt; FALSE,
        'description' =&gt; t('Clickability article image page 2'),
      ),
      'image3' =&gt; array(
        'type' =&gt; 'text',
        'not null' =&gt; FALSE,
        'description' =&gt; t('Clickability article image page 3'),
      ),
      'master_image_byline_title' =&gt; array(
        'type' =&gt; 'text',
        'not null' =&gt; FALSE,
        'description' =&gt; t('Clickability article image page 7'),
      ),
      'tags' =&gt; array(
        'type' =&gt; 'text',
        'not null' =&gt; FALSE,
        'description' =&gt; t('Clickability article image page 7'),
      ),
      'status' =&gt; array(
        'type' =&gt; 'text',
        'not null' =&gt; TRUE,
        'description' =&gt; t('Clickability article status'),
      ),
      'websitePlacements' =&gt; array(
        'type' =&gt; 'text',
        'not null' =&gt; TRUE,
        'description' =&gt; t('Clickability book review status'),
      ),
      ),
    'primary key' =&gt; array('id'),
  );
  return $schema;
}

function my_import_install() {
  $ret = drupal_install_schema('my_import');
  return $ret;
}

function my_import_uninstall() {
  $ret = drupal_uninstall_schema('my_import');
  return $ret;
}</code></pre>
</div>


<p>When I created the schema, I took care to make sure
that the column names in my table exactly matched the attributes and elements
I was looking to pull out of the XML file. This saves a lot of coding later.
Any time we change the schema, you can create a hook_update_N() function, or
just change the schema and disable+uninstall+install the custom module. I did
the latter with a drush alias and it worked well. The hook_install() and
hook_uninstall() functions simply add and remove the tables.</p>

<h2>Setup the Command Line Script to Import the XML into the DB</h2>

<p>Create the file myimport.php in your module directory, and paste in the
following:</p>

<div>
  <pre><code class='php'>#!/usr/bin/php
&lt;?php
// get the path to our XML file
$args = getopt(&quot;f:&quot;);

// Bootstrap Drupal
require_once './includes/bootstrap.inc';
drupal_bootstrap(DRUPAL_BOOTSTRAP_FULL);

// Make sure my_import is enabled
if (!module_exists('my_import')) { 
  echo &quot;I need the my_import module enabled!  Exiting.\n&quot;;
  exit(1);
}

/*
 * Make sure our media directory exists.
 * We will import from this directory into whatever directory filefield is configured for
 * so we should remove this dir when done with the migration.
 */
$media_dir = file_directory_path() .'/migrated';
echo &quot;Media dir = $media_dir\n&quot;;
if (! is_dir($media_dir)) {
  mkdir($media_dir);
}

// Slurp in our XML file.  If your XML file is huge, watch your PHP memory limits
$xml = simplexml_load_file($args['f']);
echo &quot;XML Loaded.\n&quot;;

$rowcount = 0;
// Here we iterate over each child of the root of the XML, which in our case is a Article
foreach ($xml-&gt;children() as $content) {
  // Setup our $obj object which represents a row in the DB, and use some caching to 
  // not abuse drupal_get_schema().
  $obj = new stdClass;
  static $schema = array();
  
  // Dereference our child from the parent $xml, or xpath performance sucks hard
  $content = simplexml_load_string($content-&gt;asXML());
  
  $table = NULL;
  $content_type = NULL;
  switch((string) $content['type']) { 
    // Add more case statements for more content types as needed
    case 'Article':
      $table = 'clickability_articles';
      $content_type = 'article';
      break;
    // All cases below are silently ignored - we are not importing them
    case 'Book Reviews':
    case 'Blog Topic':
    case 'Event':
    case 'Job': 
      // Ignored
      break;
    default:
      // Any content type not accounted for gets reported
      echo &quot;Warning: unknown content of type &quot;. $content['type'] .&quot;\n&quot;;
  }
  if (isset($table)) {
    if (! isset($schema[$table])) {
      // Get the table schema from Drupal
      $schema[$table] = drupal_get_schema($table);
      // On first run, truncate the table
      $sql = &quot;truncate table {$table}&quot;;
      db_query($sql);
      echo &quot;$table truncated.\n&quot;;
    }
    // This function does the heavy lifting, creating the $obj object from the XML data
    $obj = xml2object($content, $schema[$table], $content['type']); 
    // There are some cases where $obj is intentionally null, only write to the db if not null
    if ($obj) {
      $ret = drupal_write_record($table, $obj);
      if ($ret) {
        $rowcount++;
      }
    }
  }
}
echo &quot;Inserted $rowcount rows.\n&quot;;

function xml2object($xml, $tableschema, $content_type) {
  
  global $media_dir;
  $obj = new stdClass;
  // Our main iterator is the column names in the table
  foreach (array_keys($tableschema['fields']) as $field) {
    switch($field) {
      case 'master_image_byline_title':
        // This field is populated when we work with the images later on
        break;
      case 'id':
        $obj-&gt;$field = $xml[$field];
        break;
      case 'status':
        $obj-&gt;$field = (string)$xml-&gt;$field;
        break;
      // A Clickability placement roughly corresponds to a Drupal term
      case 'placement':
        $element =  array_pop($xml-&gt;xpath(&quot;//field[@name='$field']&quot;));
        $obj-&gt;$field = (string)$element-&gt;row-&gt;value;
        $obj-&gt;$field = map_taxonomy($obj-&gt;$field, $content_type);
        break;
      case 'author':
        $element =  array_pop($xml-&gt;xpath(&quot;//field[@name='$field']&quot;));
        $obj-&gt;$field = (string)$element-&gt;value;
        break;
      case 'image2':
      case 'image3':
        // Combine image2 and image3 elements in Clickability into our multivalue filefield as csv
        if ($content_type == &quot;Article&quot;) {
          $mediaplacement = array_pop($xml-&gt;xpath(&quot;//mediaPlacement[@name='$field']&quot;));
          // migrate module requires full path to filefield source
          $obj-&gt;$field = getcwd() .'/'. $media_dir .'/'. (string)$mediaplacement-&gt;media-&gt;path;
          if (substr($obj-&gt;$field, -1, 1) == '/')  {
            $obj-&gt;$field = NULL;
          }
          else {
            if (!empty($obj-&gt;image)) {
              $obj-&gt;image .= &quot;,&quot;;
            }
            $obj-&gt;image .= $obj-&gt;$field;
          }
        }
        break;
      case 'thumbnail':
      case 'image':
        $mediaplacement = array_pop($xml-&gt;xpath(&quot;//mediaPlacement[@name='$field']&quot;));
        // migrate module requires full path to filefield source
        $obj-&gt;$field = getcwd() .'/'. $media_dir .'/'. (string)$mediaplacement-&gt;media-&gt;path;
        // Check the schema.  If the field is required, then fill in a default, otherwise wipe it
        $required = $tableschema['fields'][$field]['not null'];
        // If the file path ends in a /, then the XML did not have an image for this article
        // -- if we require one, make a default
        if (substr($obj-&gt;$field, -1, 1) == '/')  {
          if ($required) {
            echo &quot;$content_type with ID of &quot;. $obj-&gt;id .&quot; does not have a $field.  Adding test.gif.\n&quot;;
            $obj-&gt;$field .= &quot;test.gif&quot;;
            touch($obj-&gt;$field);
          }
          else {
            // NOTE: We need this patch for this to work: http://drupal.org/node/780920
            $obj-&gt;$field = NULL;
          }
        }
        else {
          // Transfer the caption on the image in the XML to the CCK byline accreditation
          $obj-&gt;master_image_byline_title = (string)$mediaplacement-&gt;caption;
          // See if the file exists on the filesystem
          if (! file_exists($obj-&gt;$field)) {
            // Nope, let's fill it in with our default image
            echo $obj-&gt;$field .&quot; does not exist, replacing with test.gif.\n&quot;;
            $obj-&gt;$field = preg_replace('#^(.*)/(.*)$#', '\1/test.gif', $obj-&gt;$field);
          }
          
          // Replace .bmp with .jpg
          $jpg = preg_replace('/\.bmp$/', '.jpg', $obj-&gt;$field);
          if ($jpg != $obj-&gt;$field) {
            if (file_exists($jpg)) {
              $obj-&gt;$field = $jpg;
            }
            else {
              // Tell the user what to do to create the image and exit.
              echo &quot;ID &quot;. $obj-&gt;id .&quot; has a image of type bmp, and no jpg found on the file system.\n&quot;;
              echo &quot;Create them by running 'mogrify -format jpg /path/to/*.bmp' and re-run this script.\n&quot;;
              exit(1);
            }
            
          }
        }
        break;
      // Any DB column not explicity defined above maps cleanly with the code below
      default:
        $obj-&gt;$field = (string)array_pop($xml-&gt;xpath(&quot;//field[@name='$field']&quot;));
        break;
    }

  }
  
  // We assume it does not need imported until we prove otherwise
  $needs_imported = FALSE;
  $tags = array();
  $websitePlacements = array();
  foreach ($xml-&gt;xpath(&quot;//websitePlacement&quot;) as $websitePlacement) {
    // Only if the XML says the domain is www.newdomain.com do we need to import
    if ($websitePlacement-&gt;domain == 'www.newdomain.com') {
      $needs_imported = TRUE;
      
      // Convert the old &quot;sections&quot; into tag taxonomy
      $tags[] = substr($section, 1, strlen($section));
      
      // Grab the old URLs from websitePlacement, and place them on an array
      $section = (string)$websitePlacement-&gt;section;
      $oldurl = $section .'/'. $obj-&gt;id .'.html';
      $websitePlacements[] = $oldurl;
      
      // If we do not have a placement yet, we try to set some form of taxonomy
      if (! isset($obj-&gt;placement)) {
        $taxo = map_taxonomy($section, $content_type);
        // NOTE: We need this patch for this to work: http://drupal.org/node/780920
        $obj-&gt;placement = $taxo;
      }
      // If the XML did not explicity tell us the createDate, we use the start date from the webSitePlacement
      if (empty($obj-&gt;createDate)) {
        $date = (string)$websitePlacement-&gt;startDate;
        $obj-&gt;createDate = substr($date, 0, strlen($date) -4);
        $obj-&gt;editDate = $obj-&gt;createDate;
      }
    }    
  }
  $obj-&gt;websitePlacements = implode(',', $websitePlacements);
  $obj-&gt;tags = implode(',', $tags);
  
  // Return the object only if we need it imported
  return $needs_imported ? $obj : NULL;
}

function map_taxonomy($oldtext, $content_type) {
  // Simple maps of Clickability placements to Drupal terms
  if ($content_type == 'Job') {
    return NULL;
  }
  if (preg_match('/building/i', $oldtext)) {
    return &quot;Green Building&quot;;
  }
  if (preg_match('/(clean|energy)/i', $oldtext)) {
    return &quot;Clean Energy&quot;;
  }
  if (preg_match('/financ/i', $oldtext)) {
    return &quot;Finance&quot;;
  }
  if (preg_match('/food/i', $oldtext)) {
    return &quot;Food &amp; Farms&quot;;
  }
  if (preg_match('/marketing/i', $oldtext)) {
    return &quot;Green Marketing&quot;;
  }
  if (preg_match('/recycled/i', $oldtext)) {
    return &quot;Recycled Markets&quot;;
  }
  if (preg_match('/technol/i', $oldtext)) {
    return &quot;Technology&quot;;
  }
  if (preg_match('/leaders/i', $oldtext)) {
    return &quot;Business Leaders&quot;;
  }
  if (preg_match('/transportation/i', $oldtext)) {
    return &quot;Transportation&quot;;
  }
  return NULL;
}

?&gt;</code></pre>
</div>


<p>Wow, that&#8217;s a lot of code. I&#8217;ve commented
it pretty heavily, but here&#8217;s the &#8220;40,000 foot view&#8221; of what&#8217;s going on:</p>

<ul>
<li><strong>Lines 1-26:</strong> Nothing too fancy here. I should note that the script expects to be executed from your Drupal root directory. It grabs the path to the XML file from the command line and does some sanity checking.</li>
<li><strong>Lines 28-30:</strong> Here we use PHP&#8217;s <a href="http://php.net/manual/en/book.simplexml.php">SimpleXML API</a> to load the entire XML file into an object. If you have a huge XML file and/or small PHP memory limits, you will likely have to use XML Parser or another library. The power and convenience of SimpleXML is a pretty convincing argument to temporarily upping your memory limits in this case.</li>
<li><strong>Lines 34-82:</strong> This is the main loop which iterates over each Article in the XML file. By looking at the content type in the XML record, we determine what table and content type to use for Drupal. The first time a schema is loaded, we truncate the table in the database. Once we determine some metadata about the record, we call xml2object() on line 72 which does most of the work for us. Once we have an object, we store it to the database.</li>
<li><strong>Lines 84-222:</strong> Here we have the xml2object() function, and yes, it&#8217;s way too long and should be broken up. But hey, it&#8217;s migration code, who else will ever see it??? We&#8217;ll break it down more below.</li>
<li><strong>Lines 89-182:</strong> This code runs a for loop around each column in the table. Since we&#8217;re using the Schema API here, we can safely assume that the column order specified in our install file will be duplicated when we fetch it in our script. For each column type in the table, it attempts to pull the data needed from the XML record, transform it if necessary, and store it in our $obj object. Read the code for details on what is happening to each field on the way in.</li>
<li><strong>Lines 184-218:</strong> Now that we have iterated over all the fields in the schema, we can use the data stored in $obj to calculate other fields we need. Again, read the code for details, but here we are setting taxonomy terms, URLs for use with path_redirect, and filling in other data that may have been missing from the XML.</li>
<li><strong>Lines 224-257:</strong> Is a simple example on how to statically map some data in the XML to return taxonomy terms in Drupal
Now that we&#8217;ve got that out of the way, let&#8217;s create our module file.</li>
</ul>


<h2>Create my_import.module</h2>

<p>Now, create a file in your module directory named my_import.module. This file
will contain the actual module used by Drupal and will implement some of the
migrate modules hooks. You might ask, why not deal with everything in the
command line script? There are two primary reasons why:</p>

<ol>
<li>You may come across a condition where you need the nid of the node (i.e. create path redirects), or otherwise interact with the $node object. You can only get this information from implementing migrate&#8217;s hooks.</li>
<li>While I personally found it easier to manipulate taxonomy terms via the command line script and then rely upon the out-of-the-box code supplied with migrate to setup the node for me, this has a drawback. Any time you change the command line script, you must &#8220;clear&#8221; your imported data, re-run the command line script, and re-import your data using the migrate module. If you make changes to your module, you only have two steps to test (clear and re-import).
Paste this code into my_import.module:</li>
</ol>


<div>
  <pre><code class='php'>&lt;?php
define(NUM_PARAGRAPHS_PER_PAGE, 6);

function my_import_migrate_prepare_user(&amp;$user, $tblinfo, $row) {
  // Randomly assign passwords to users, forcing them to reset their password
  $errors = array();
  $user['pass'] = preg_replace(&quot;/([0-9])/e&quot;,&quot;chr((\\1+112))&quot;,rand(100000,999999));
  return $errors;
}

function my_import_migrate_prepare_node(&amp;$node, $tblinfo, $row) {
  $errors = array();
  // In Clickability, there were multiple states that represented &quot;Published&quot;, here we map them.
  $status = $tblinfo-&gt;view_name .'_status';
  switch($row-&gt;$status) {
    case 'live':
    case 'APPROVED':
      $node-&gt;status = 1;
      break;
    default:
      $node-&gt;status = 0;
      break;
  } 
  
  if ($node-&gt;type == 'article') {
    // Paginate articles by inserting a pagebreak tag every 6th paragraph to emulate Clickability's pagination
    $paragraphs = preg_split('#&lt;br /&gt;\s*&lt;br /&gt;#s', $node-&gt;body);
    if (count($paragraphs) &gt; NUM_PARAGRAPHS_PER_PAGE) {
      $node-&gt;body = '';
      $i = 1;
      foreach ($paragraphs as $paragraph) {
        if (($i % NUM_PARAGRAPHS_PER_PAGE) == 0) {
          $node-&gt;body .= $paragraph . &quot;\n[pagebreak]\n&quot;;
        }
        else {
          $node-&gt;body .= $paragraph .&quot;&lt;br /&gt;\n&lt;br /&gt;\n&quot;;
        }
        $i++;
      }
    }
    
  }
  return $errors;
}

function my_import_migrate_complete_node(&amp;$node, $tblinfo, $row) {
  $errors = array();
  // Create redirects for old URLs
  $field = $tblinfo-&gt;view_name .'_websitePlacements';
  foreach(explode(',', $row-&gt;$field) as $oldurl) {
    // Delete any old redirects
    if (substr($oldurl,0,1) == '/') {
      $oldurl = substr($oldurl,1);
    }
    path_redirect_delete(array('source' =&gt; $oldurl));
    $redirect = array(
      'source' =&gt; $oldurl,
      'redirect' =&gt; '/node/'. $node-&gt;nid,
      'type' =&gt; 301,
    );
    path_redirect_save($redirect);
  }
  return $errors;
}</code></pre>
</div>


<p>Here&#8217;s the high-level breakdown, check the code+comments for
the details.</p>

<ul>
<li><strong>Lines 5-10:</strong> Just a quick example of how to set a random password on any user that is imported.</li>
<li><strong>Lines 12-24:</strong> hook_migrate_prepare_node() is executed before the node has been saved to the database, and should be where the majority of your code is at. These lines set any article with a status of &#8216;live&#8217; or &#8216;APPROVED&#8217; to published in Drupal.</li>
<li><strong>Lines 26-41:</strong> This code uses some regex magic to create a pagebreak every 6th paragraph. This is what Clickability did, and the client wanted to keep this on their migrated articles.</li>
<li><strong>Lines 47-65:</strong> hook_migrate_complete_node() is called after the node has been saved to the database, and it has a nid at this point. The client wished to migrate their old URL&#8217;s to Drupal &#8211; in order to do that we must have the nid to know where to redirect to.</li>
</ul>


<h2>Create sample XML</h2>

<p>Finally, let&#8217;s create some sample data so we can see how this all meshes
together. Create the file content.xml in your module directory, and paste this
in it:</p>

<div>
  <pre><code class='xml'>&lt;?xml version=&quot;1.0&quot; encoding=&quot;utf-8&quot;?&gt;
&lt;cmPublishImport&gt;
  &lt;content type=&quot;Article&quot; id=&quot;7241321&quot;&gt;
    &lt;field name=&quot;title&quot;&gt;&lt;![CDATA[Donec risus purus]]&gt;&lt;/field&gt;
    &lt;field name=&quot;author&quot;&gt;
      &lt;value&gt;&lt;![CDATA[Me]]&gt;&lt;/value&gt;
    &lt;/field&gt;
    &lt;field name=&quot;articleauthor&quot;&gt;&lt;![CDATA[Me]]&gt;&lt;/field&gt;
    &lt;field name=&quot;date&quot;&gt;&lt;![CDATA[2007-04-29]]&gt;&lt;/field&gt;
    &lt;field name=&quot;summary&quot;&gt;&lt;![CDATA[Donec risus purus]]&gt;&lt;/field&gt;
    &lt;field name=&quot;body&quot;&gt;&lt;![CDATA[Donec risus purus, euismod eu volutpat ac, pharetra non nulla. Vestibulum quis neque lacus. Donec sit amet tortor nisi. Nam et lectus nec turpis consequat rhoncus. Proin porttitor, quam nec faucibus pulvinar, arcu magna facilisis erat, eu imperdiet risus tortor ac quam. Praesent non justo ac nisl ultricies condimentum a eget arcu. Nam in mi est. Donec risus orci, imperdiet ut tempus et, pulvinar nec diam. Donec eleifend pulvinar aliquam. Nulla faucibus turpis nec neque scelerisque convallis. Fusce gravida pulvinar quam, sit amet faucibus risus sodales ornare. Nullam arcu risus, lacinia vel faucibus at, auctor eget diam. Quisque a neque ac tellus bibendum luctus fringilla in lacus. Praesent id nunc eu dolor adipiscing consequat vel eget leo. Donec velit mi, pharetra quis tincidunt id, laoreet et dolor. Vestibulum fringilla rutrum arcu at accumsan.&lt;br/&gt;
&lt;br/&gt;  
Cras pellentesque sagittis mi. Pellentesque cursus nisl id nunc suscipit luctus. Duis pellentesque rhoncus sodales. Nullam dictum augue ac diam fermentum vel feugiat mauris euismod. Mauris nec metus eu sem tristique euismod. Etiam lorem est, accumsan vitae bibendum sit amet, tempus sit amet urna. Nullam lobortis adipiscing convallis. Nullam scelerisque sagittis tellus vitae interdum. Integer eget interdum nunc. Nam ligula orci, bibendum ac mattis eget, mollis at massa.&lt;br/&gt;
&lt;br/&gt;  
Vestibulum sodales elit vel est feugiat vitae dapibus erat ultricies. Proin auctor quam sit amet nisi aliquet pharetra. Curabitur tristique quam vel tortor gravida scelerisque. Morbi laoreet aliquet mi, sed imperdiet mauris mattis et. Praesent non quam nec lorem dapibus semper. Quisque vulputate neque et turpis placerat bibendum. Phasellus suscipit urna eget augue ullamcorper ultricies. Curabitur hendrerit dui sit amet elit elementum nec venenatis orci tempor. Fusce semper vestibulum odio vitae porta. Mauris non tellus non mi hendrerit suscipit in sed ante. Donec arcu neque, tristique ut elementum sed, suscipit at leo. Curabitur eget enim quis leo scelerisque laoreet et eget augue. Fusce posuere est ac felis fringilla consectetur. Nulla elit magna, pharetra sit amet tincidunt sed, tristique sed mi. Nam iaculis, elit sit amet condimentum blandit, massa neque pharetra justo, non ornare ligula ante non leo. Praesent ullamcorper suscipit tempus. In varius, neque eget volutpat posuere, velit odio luctus turpis, ac varius nulla erat sit amet justo. Quisque convallis mollis pharetra. Aliquam porta dolor quis nunc tempor vitae pharetra lectus ultricies. Fusce egestas sagittis sapien, sit amet pharetra sem ullamcorper a.&lt;br/&gt;
&lt;br/&gt;  
Ut dui tortor, porta eu ultrices sed, interdum vitae lectus. Integer facilisis velit sit amet dui ultricies lobortis. Fusce ut malesuada tellus. Aenean in nibh at lorem iaculis dictum vitae in nulla. Etiam dapibus lacinia eleifend. Aliquam erat volutpat. Nullam sit amet sapien ut risus consequat posuere eu quis quam. In lobortis fringilla felis quis pretium. Suspendisse non nisl libero, non tempor justo. Nunc volutpat nulla vitae lacus tincidunt feugiat congue sapien commodo. Suspendisse venenatis aliquet ante in hendrerit. Sed lectus ligula, gravida id tincidunt et, feugiat non justo.&lt;br/&gt;
&lt;br/&gt;  
Sed metus tellus, vestibulum in mollis quis, imperdiet et velit. Praesent suscipit elit et mi rutrum sit amet gravida augue iaculis. Etiam nec tellus nec augue porttitor pharetra. Vivamus feugiat mollis est, eu aliquam neque tempus a. Ut magna mauris, sollicitudin in ornare non, lacinia a lacus. Aenean porttitor magna ac sem ornare pellentesque. Aliquam mattis dolor in metus molestie ut feugiat mi auctor. Etiam laoreet pulvinar ipsum id bibendum. Class aptent taciti sociosqu ad litora torquent per conubia nostra, per inceptos himenaeos. Quisque porttitor convallis lacus, nec pretium leo varius non. Morbi non dapibus diam. Sed nec venenatis diam. Cras mollis porta tempor.&lt;br/&gt;
&lt;br/&gt;  
Donec ornare mi sed tellus porta luctus. Nulla euismod venenatis ante, in rhoncus felis ornare non. Cras tempor venenatis est at gravida. Etiam imperdiet dolor vitae ipsum lacinia imperdiet. Maecenas purus lorem, rhoncus non porttitor in, semper nec quam. Integer ullamcorper facilisis ultrices. Vivamus porttitor lacinia augue in venenatis. Quisque interdum euismod tellus, et consectetur nunc dictum sit amet. Maecenas pulvinar placerat mauris, quis auctor purus pellentesque at. Vestibulum vulputate, tellus id eleifend posuere, ligula erat hendrerit orci, nec lobortis tortor sapien ut ligula. Donec id augue leo, non consectetur nisl. In viverra dictum lorem eget blandit. Etiam tempus nisl ac nibh viverra id cursus eros luctus. Duis ut tellus nisi.&lt;br/&gt;
&lt;br/&gt;  
Class aptent taciti sociosqu ad litora torquent per conubia nostra, per inceptos himenaeos. Aliquam quis justo risus, eget eleifend nibh. Morbi quis dolor nulla, sed cursus metus. Vestibulum vel ipsum non erat tincidunt luctus et eget sapien. Nunc vel justo ante, vel auctor purus. Proin vulputate bibendum placerat. Fusce vel tincidunt nunc. Praesent at eros in dolor faucibus blandit et vitae magna. Fusce arcu nisl, sollicitudin sed accumsan sed, rhoncus at tellus. Ut ut mauris vel ipsum bibendum ullamcorper eget sed neque. Class aptent taciti sociosqu ad litora torquent per conubia nostra, per inceptos himenaeos. Sed dui massa, imperdiet sit amet lacinia id, rutrum sed orci. Proin pharetra risus eu risus gravida convallis id ac mi. Etiam a neque ut lacus convallis accumsan non eget arcu. Sed blandit velit id lectus tincidunt ut aliquet mi egestas. Aliquam cursus odio vitae turpis suscipit mollis et aliquam purus. Suspendisse pretium tincidunt porttitor. Nunc vestibulum, lacus at auctor laoreet, orci lectus volutpat diam, ut mollis risus lectus a ligula.&lt;br/&gt;
&lt;br/&gt;  
Phasellus id urna sit amet elit pretium viverra sit amet eget felis. Maecenas sed arcu sed eros fringilla commodo id non magna. Maecenas urna mauris, cursus vel mattis et, volutpat in purus. Nulla sapien orci, faucibus sed tincidunt tincidunt, tristique id lacus. Nullam tortor libero, porttitor eget faucibus eget, vehicula a enim. Suspendisse malesuada consectetur mattis. Integer dapibus dignissim tempor. In viverra luctus orci sed placerat. Suspendisse aliquam mattis diam mattis dapibus. Aenean suscipit purus eu ipsum dignissim in aliquet urna mollis. Duis nibh magna, fringilla eu ultrices posuere, sodales sed felis. Proin varius dignissim sem a consequat. Pellentesque facilisis felis vel mi malesuada placerat. Curabitur gravida euismod mi in molestie. Vivamus sit amet dui leo. Praesent mi justo, bibendum at rutrum ac, bibendum ut felis. Nullam nec dolor dui, quis imperdiet nulla. Morbi semper pulvinar risus.]]&gt;&lt;/field&gt;
    &lt;mediaPlacement name=&quot;image&quot;&gt;
      &lt;media id=&quot;577286&quot;&gt;
        &lt;path&gt;images/1.jpg&lt;/path&gt;
        &lt;caption&gt;&lt;![CDATA[Cows at Three Mile Canyon provide resources such as methane and compost for on-farm operations.]]&gt;&lt;/caption&gt;
      &lt;/media&gt;
    &lt;/mediaPlacement&gt;
    &lt;status&gt;live&lt;/status&gt;
    &lt;websitePlacement&gt;
      &lt;domain&gt;www.newdomain.com&lt;/domain&gt;
      &lt;section&gt;/foodandfarms&lt;/section&gt;
      &lt;startDate dateFormat=&quot;yyyy-MM-dd HH:mm:ss zzz&quot;&gt;2007-04-29 14:00:00 PDT&lt;/startDate&gt;
    &lt;/websitePlacement&gt;
    &lt;websitePlacement&gt;
      &lt;domain&gt;www.olddomain.com&lt;/domain&gt;
      &lt;section&gt;/greenmarketing&lt;/section&gt;
      &lt;startDate dateFormat=&quot;yyyy-MM-dd HH:mm:ss zzz&quot;&gt;2007-04-29 14:00:00 PDT&lt;/startDate&gt;
    &lt;/websitePlacement&gt;
  &lt;/content&gt;
&lt;/cmPublishImport&gt;</code></pre>
</div>


<h2>Enable the my_import Module and Run the Command Line Script</h2>

<p>Now (finally), it&#8217;s time for some action. Enable your newly created my_import
module, and jump out to the shell. Assuming your Drupal root is at
/var/www/drupal, cd into that directory. Create the new directory
sites/default/files/migrated/images, and place a jpg named 1.jpg in that
directory. Now run the import script:</p>

<pre><code>php5 ./sites/all/modules/my_import/myimport.php  -f ./sites/all/modules/my_import/content.xml
</code></pre>

<p>With luck, the script will succeed, and you will have 1 row of data in your
clickability_articles table! If not, fix the error (if you&#8217;re using the sample
data, let me know what went wrong and I&#8217;ll fix it). Next up, Table Wizard
configuration.</p>

<h2>Expose the Table to Table Wizard</h2>

<p>All the hard work is done now - we can use a web UI from here on out. Visit
/admin/content/tw in your browser, and under the &#8220;Add Existing Tables&#8221;
fieldset, and select the tables you imported with myimport.php. If your tables
are huge (50K+ rows), you may want to select &#8220;Skip full analysis&#8221;. Click the
&#8220;Add tables&#8221; button. At this point, that&#8217;s all we need from Table Wizard, but
I strongly encourage you click around a bit. The table analysis can tell you
some handy things about your data.</p>

<h2>Create the Migrate Content Set</h2>

<p>In the previous step, we essentially built a view that we can provide to the
Migrate module. Now we need to tell Migrate how to use the view. Visit the
Migrate settings at /admin/content/migrate/settings. If you can, implement the
changes it recommends to .htaccess as it will speed up the import
considerably. Also, make sure to expand the &#8220;Migration support implemented in
the XYZ module&#8221; fieldsets and enable the support you need for your import.
Now, visit the dashboard at /admin/content/migrate. Expand the &#8220;Add a content
set&#8221; fieldset, and fill in the values. When choosing the value for &#8220;Source
view from which to import content&#8221;, scroll down towards the bottom of the
list. All Table Wizard views are prefixed with &#8220;tw: &#8220;, so the one we&#8217;re
looking for here is &#8220;tw: clickability_issues (clickability_issues)&#8221;. You can
leave &#8220;View arguments&#8221; and &#8220;Weight&#8221; to defaults. The next screen is where the
real magic happens. By interrogating the view, Migrate presents you with a map
fields form that allows us to select our source column from a dropdown to
assign to various node elements. If you have a setting that should remain
constant across all imported records (&#8220;Node: Input format&#8221; is usually a good
example), you can type in a default value here. The rest should be fairly self
explanatory. Click &#8220;Submit changes&#8221;, and you&#8217;ll be taken back to the
dashboard.</p>

<h2>Run the Import, Clear the Import, Wash, Rinse, Repeat</h2>

<p>Now, the way I did my testing was to choose one row from the source table to
import. Grab its primary key and copy it to the clipboard. Check the box under
&#8220;Import&#8221; for our content set, then expand the &#8220;Execute&#8221; fieldset. Paste the ID
into the &#8220;Source IDs:&#8221; text field, and click the Run button. With any luck,
you will be returned to the dashboard, but the content set will show 1
imported. Hopefully there will be no errors, but if there are, find and fix
the problem. You can view the old primary key mapping to the node ID by going
back to /admin/content/tw and looking for a view named
migrate_map_si_articles. This table is created by the Migrate module &#8211; it
uses this table to track what has been imported, and what NID the imported
nodes have. Grab that nid, and load up /node/[nid]. If it looks good, then we
can to a bigger import. Go back to the Migrate dashboard, and this time click
the &#8220;Clear&#8221; checkbox next to the content set. Expand &#8220;Execute&#8221;, make sure all
fields are blank, and click the Run button. This process will &#8220;unimport&#8221; the
row we just imported. Now, depending on your row count, you may want to import
all rows and see what happens. Since I was dealing with thousands of nodes, I
did an import of just 100 nodes to make sure things were okay. To do this,
instead of specifying &#8220;Source IDs&#8221;, place the number 100 in &#8220;Sample Size&#8221;, and
click Run. To import everything, leave all fields blank. The power to quickly
and easily remove all changes made by the migrate module is huge. Because of
this &#8220;safety net&#8221;, it lets you work on the import within the same development
sandbox as your designers and themers. They&#8217;ll appreciate having something
other than &#8220;Lorem Ipsum&#8221; to look at!</p>

<h2>Run to the Nearest Pub and Celebrate the Completion of Your Migration</h2>

<p>If I have to explain this to you, you&#8217;re in the wrong field of work!</p>

<h2>Summary</h2>

<p>This post is my longest to date, and there&#8217;s a good chance I missed some
things. By all means, let me know in the comments if you find any holes and
I&#8217;ll get it corrected. I hope this case study helps some other Drupalers out
there - when I first started this project I couldn&#8217;t find any examples on how
to get XML into Drupal using the Migrate module. Now Google has some spider
food :)</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[My Thoughts and Ramblings on DrupalConSF 2010]]></title>
    <link href="http://sysadminsjourney.com/content/2010/04/23/my-thoughts-and-ramblings-drupalconsf-2010"/>
    <updated>2010-04-23T00:00:00-05:00</updated>
    <id>http://sysadminsjourney.com/content/2010/04/23/my-thoughts-and-ramblings-on-drupalconsf-2010</id>
    <content type="html"><![CDATA[<p>I had the great pleasure of attending my first
<a href="http://sf2010.drupal.org">DrupalCon</a> this week. Held in downtown San
Francisco at the Moscone Center, it was my opinion that this was Drupal&#8217;s
&#8220;homecoming&#8221;. While Drupal wasn&#8217;t &#8220;born&#8221; in San Francisco, it seems to be the
city that has the strongest following. The attendance numbers didn&#8217;t lie - I&#8217;m
pretty sure they broke 3,000 <del>geeks</del> attendees. I made this trip solo
&#8211; I only knew three people that were going, and those three were only
acquaintances I&#8217;d met via email/IM a few months before. When I left, I didn&#8217;t
come home with &#8220;leads&#8221; or &#8220;contacts&#8221;, I came home with friends and role
models, many of whom I plan on staying in touch with. I met most of the
authors of the Drupal books I&#8217;ve read, associated faces to the podcasts and
RSS feeds I subscribe to, and I even had the opportunity to quickly say thanks
to Dries and shake his hand.</p>

<p>For those who didn&#8217;t know, archive.org has made the sessions <a href="http://www.archive.org/search.php?query=DrupalCon&amp;sort=-publicdate">available for
download</a>,
so be sure to check those out. Read on for my &#8220;takeaways&#8221; from DCSF2010.</p>

<p>Please note that these are just what come to my mind, I&#8217;m sure I&#8217;m forgetting
huge topics. Please forgive me in advance for those!</p>

<h2>Larry Garfield is my favorite presenter of the conference</h2>

<p><a href="http://drupal.org/user/26398">Larry Garfield</a> works for
<a href="http://palantir.net/">Palantir.net</a>, and is one of the few people that I&#8217;ve
listened to that is immensely intelligent, yet speak well and even make a
crowd genuinely laugh out loud. I attended his &#8220;Objectifying PHP&#8221; and &#8220;Views
for Developers&#8221; sessions, and left feeling motivated and enlightened. My
thanks go out to him, as he very obviously put a lot of preparation time into
his presentations.</p>

<h2>Drupal is methodically (pun intended) implementing OO</h2>

<p>As evidenced by Larry Garfield&#8217;s &#8220;Objectfying PHP&#8221; and John VanDyk&#8217;s &#8220;Batch vs
Queue&#8221; session, Drupal is refactoring portions of core into classes and
methods where it fits. I&#8217;m part of the camp that welcomes the change, and
can&#8217;t wait. I can&#8217;t help but wonder if we&#8217;ll alienate some contrib module
authors in the process, but I&#8217;m sure that it will bring the overall quality of
contrib modules up a few notches.</p>

<h2>David Strauss knows what he&#8217;s talking about</h2>

<p>I&#8217;ve been in IT/Networking/Programming/etc for about 20 years now. While I
don&#8217;t claim to be the smartest person in the group at any point in time, I
consider myself pretty well rounded. It&#8217;s been a long time since someone was
able to truly talk so far over my head that I couldn&#8217;t keep up, but <a href="http://fourkitchens.com/bios/david-strauss">David
Strauss</a> of <a href="http://fourkitchens.com/">Four
Kitchens</a> did just that at the <a href="http://chapterthree.com">Chapter Three
</a>open house party. We discussed HipHop PHP, operating
systems, configuration management, and god knows what else. I had to look like
a deer in headlights!</p>

<h2>HipHop PHP will eventually run Drupal</h2>

<p>I can say this because David Strauss is the one working on it. Enough said.</p>

<h2>Microsoft is playing it smart</h2>

<p>Instead of trying to compete with Drupal, they&#8217;re finally trying to help
Drupal. I&#8217;m a hardcore anything-but-Microsoft OS kinda guy, but I can&#8217;t
dispute that there&#8217;s a lot of shops out there that already have well versed
SQL Server and IIS admins. Microsoft announced that they now have a native SQL
Server driver for PHP, and that Drupal can now run on MS SQL Server. This will
be a huge boon for getting Drupal into the Microsoft-centric enterprises -
there&#8217;s no longer a need to have a MySQL guy. Oh, and <a href="http://www.sysadminsjourney.com/content/2010/04/20/2010-what-year">giving away free
alcohol</a>
never hurt either :)</p>

<h2>MongoDB will have a large impact on Drupal 7</h2>

<p><a href="http://drupal.org/user/9446">Chx</a> gave an excellent presentation - &#8221;<a href="http://sf2010.drupal.org/conference/sessions/mongodb-%0Ahumongous-drupal">MongoDB:
Humongous Drupal</a>&#8221;. He covered a lot about SQL, and how over the years it&#8217;s
become &#8220;best practice&#8221; to de-normalize tables to improve performance. We&#8217;ve
all done that, but have you ever pondered that you&#8217;re breaking one of the
fundamental rules of relational databases when you&#8217;ve done that? While MongoDB
is perfectly suited for logging and caching in Drupal, the biggest win is with
Fields in Drupal 7. Each field you create results in a new table that must be
added to a JOIN when building a node. Shops with a lot of fields on their
nodes will likely see huge gains in performance by moving to MongoDB for those
tables.</p>

<h2>Big Drupal is Big</h2>

<p>Hey, did you hear that Drupal powers whitehouse.gov? Seriously, there&#8217;s been a
lot of progress in the past year with regards to making Drupal scale. <a href="http://getpantheon.com/mercury/what-is-mercury">Project
Mercury</a> from the great folks
at <a href="http://www.chapterthree.com">Chapter Three</a> makes Big Drupal easy, and is
now supported on Amazon&#8217;s EC2, Rackspace, and
<a href="http://www.linode.com/?r=c4f79463ba583ec1f15e3307190bda4bda9d65df">Linode</a>
<shameless_plug>thanks to my
<a href="http://www.sysadminsjourney.com/content/2010/04/12/new-linode-%0Astackscript-pantheon-mercury-high-performance-drupal-10-minutes-or-%0Aless">stackscript</a></shameless_plug>. There was a huge amount of interest in Mercury and how
it all works at the conference. The BOF session was great - unfortunately I
missed the sessions where it was discussed in more detail.</p>

<h2>Chapter Three rocks</h2>

<p>Two out of the three people I knew coming into DCSF currently work for Chapter
Three, and the third person used to work for them. Special thanks to <a href="http://www.chapterthree.com/about_us/greg_coit">Greg
Coit</a> and <a href="http://uptownalmanac.com/users/kmonty">Kevin
Montgomery</a> for taking me under their
wing and introducing me to all their colleagues. I also had the pleasure to
meet <a href="http://www.chapterthree.com/blog/josh_koenig">Josh Koenig</a>, albeit
briefly. Seems the partner/CTO of one of the leading Drupal shops is a little
busy at a DrupalCon. I ended up meeting a few other guys that I clicked really
well with and hope to keep tabs on: Jeff Graham of
<a href="http://www.funnymonkey.com/">FunnyMonkey</a>, Rob Wohleb of
<a href="http://www.xomba.com/">Xomba.com</a>, and Aaron Levy of <a href="http://www.chapterthree.com">Chapter
Three</a> - thanks for the beer and discussion!</p>

<h2>Git will change Drupal.org</h2>

<p>The migration to Git can&#8217;t happen fast enough for me. Aside from the ability
to commit code on a plane, contrib modules will benefit greatly. When all is
said and done, every new issue on drupal.org will have it&#8217;s own repository
that any user will be able to commit to. Once the issue is resolved, the fix
will be merged back into the main module repo. That should break down even
more barriers for new contrib authors getting into Drupal development.</p>

<h2>Dries Buytaert is really tall</h2>

<p>Yes, Dries is very tall - at least 6&#8217;4&#8221; if I had to guess, but this is
actually just a way for me to remind you that I shook Dries&#8217; hand :) I was
more than a little starstruck!</p>

<p>Overall, I had a blast, and can&#8217;t wait for the next DrupalCon in the states. I
heard it&#8217;s in Chicago &#8211; count me in! If you ever get the chance, I absolutely
recommend that you attend.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[2010: What a Year!]]></title>
    <link href="http://sysadminsjourney.com/content/2010/04/20/2010-what-year"/>
    <updated>2010-04-20T00:00:00-05:00</updated>
    <id>http://sysadminsjourney.com/content/2010/04/20/2010-what-a-year</id>
    <content type="html"><![CDATA[<p>If you would have told me 5 years ago that:</p>

<ol>
<li>Sun would be gone</li>
<li>I would take personal time off from work to attend DrupalCon (and not regret it)</li>
<li>I would attend a Microsoft party
I would have told you where to shove it. Well:
<img src="http://lh3.ggpht.com/_FC5vt0s6_v8/S82311L-%0ATtI/AAAAAAAAMNw/ACAF0hhPT_s/s288/IMG_0272.JPG" alt="" /> For all those wondering, I went
to the party as a saboteur on a mission: to drink as much as I could to try
and directly impact their bottom line. Mission accomplished :) Good times @
DCSF2010!</li>
</ol>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Ask SAJ: Any SA's at DrupalConSF?]]></title>
    <link href="http://sysadminsjourney.com/content/2010/04/18/ask-saj-any-sas-drupalconsf"/>
    <updated>2010-04-18T00:00:00-05:00</updated>
    <id>http://sysadminsjourney.com/content/2010/04/18/ask-saj-any-sas-at-drupalconsf</id>
    <content type="html"><![CDATA[<p>It snuck up on me so fast, I forgot to ask - are any of my SysAdmin&#8217;s Journey
readers going to San Franciso for DrupalCon? If so, <a href="http://sysadminsjourney.com/contact">contact
me</a>, and we&#8217;ll see if we can meet up for
a beer! I&#8217;m posting this from DIA, so if you happen to be here and want to
grab a bite to eat, ping me. I&#8217;m flying out to SF at about 12:30pm MDT.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[New Linode StackScript: Pantheon Mercury (High Performance Drupal in 10 Minutes or Less)]]></title>
    <link href="http://sysadminsjourney.com/content/2010/04/12/new-linode-stackscript-pantheon-mercury-high-performance-drupal-10-minutes-or-less"/>
    <updated>2010-04-12T00:00:00-05:00</updated>
    <id>http://sysadminsjourney.com/content/2010/04/12/new-linode-stackscript-pantheon-mercury-high-performance-drupal-in-10-minutes-or-less</id>
    <content type="html"><![CDATA[<p>For those who might not know, <a href="http://www.getpantheon.com/mercury/what-is-mercury">Pantheon
Mercury</a> is:</p>

<blockquote><p>&#8230; a drop-in replacement for your Drupal website hosting service that
delivers break-through performance. Mercury can serve two-hundred times more
pages per second and generate pages three times faster than standard hosting
services.</p></blockquote>

<p>Mercury achieves this by using open-source technologies like so many
ingredients of a complex dish - a little <a href="http://varnish-%0Acache.org/wiki/LoadBalancing">Varnish</a> here, a dash of
<a href="http://memcached.org/">Memcached</a> there, a hint of <a href="http://php.net/manual/en/book.apc.php">the Alternative PHP
Cache</a>, a healthy dose of
<a href="http://tomcat.apache.org">Tomcat</a> and <a href="http://lucene.apache.org/solr/">Solr</a>,
all based upon the <a href="http://fourkitchens.com/pressflow-makes-drupal-%0Ascale">Pressflow</a> distribution of <a href="http://drupal.org">Drupal</a>. None of it is anything you
couldn&#8217;t do yourself &#8211; many before <a href="http://www.chapterthree.com">Chapter
Three</a> had done it actually. However, they were
the first to tie it all together using
<a href="http://trac.mcs.anl.gov/projects/bcfg2">BCFG2</a>, and release an Amazon EC2 AMI
image of it. As word spread, many liked the idea of Mercury, but wanted to
brew their own non-EC2 instance. While they <a href="http://groups.drupal.org/node/50408">posted a wiki
article</a> on how to do it yourself, they
went to work on native support for <a href="http://www.rackspace.com">RackSpace</a>. When
I read <a href="http://www.chapterthree.com/blog/josh_koenig">Josh Koenig</a>&#8217;s post on
the
<a href="http://www.linode.com/?r=c4f79463ba583ec1f15e3307190bda4bda9d65df">Linode</a>
blog stating <a href="http://blog.linode.com/2010/02/09/introducing-%0Astackscripts/#comment-40480?r=c4f79463ba583ec1f15e3307190bda4bda9d65df">he wanted to bring Mercury to
Linode</a>, I
made a mental note. Some time passed, I became much more involved in Drupal,
and I decided to volunteer to write the <a href="http://www.linode.com/stackscripts/view/?StackScriptID=353&amp;r=c4f79463ba583ec1f15e3307190bda4bda9d65df">StackScript</a>
. Josh said okay, and put me in touch with <a href="http://www.chapterthree.com/blog/greg_coit">Greg
Coit</a>, their resident sysadmin,
and we went to work. Fast forward a couple weeks, and we&#8217;ve announced a beta!
<a href="http://www.linode.com/stackscripts/view/?StackScriptID=353&amp;r=c4f79463ba583ec1f15e3307190bda4bda9d65df">The StackScript</a> is quite complete - it supports
Ubuntu Jaunty and Karmic, and can use the current stable branch or the soon-
to-be-released 1.1 development branch. Once Lucid is released, we&#8217;ll test to
make sure it works there as well. I want to thank Greg for all his help. We
found some bugs in Ubuntu, some quirks in the memcached init script, and fixed
many bugs and added some features to <a href="https://edge.launchpad.net/pantheon/bcfg2">their BCFG2 bazaar
repo</a>. Thanks also go out to Josh
for his oversight and guidance. It was a great time, a great learning
experience, and I came out of it with some new colleagues (and some free beers
at <a href="http://sf2010.drupal.org">DrupalConSF</a>). Feel free to <a href="http://www.sysadminsjourney.com/category/linode">read up on my
experiences with Linode</a>, and
if you like what you see, click on <a href="http://www.linode.com/?r=c4f79463ba583ec1f15e3307190bda4bda9d65df">one of the many links to
Linode</a>
from my blog. If you sign up and stay a customer for 90 days (trust me, you
will), I&#8217;ll get $20 credited to my account. Feel free to comment below about
the StackScript and let me know about any issues you might find.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Tracking Drupal's Outbound HTTP Requests using tcpdump]]></title>
    <link href="http://sysadminsjourney.com/content/2010/03/11/tracking-drupals-outbound-http-requests-using-tcpdump"/>
    <updated>2010-03-11T00:00:00-06:00</updated>
    <id>http://sysadminsjourney.com/content/2010/03/11/tracking-drupals-outbound-http-requests-using-tcpdump</id>
    <content type="html"><![CDATA[<p>While working on tweaking performance for a client, I was able to
shave 7 seconds of PHP execution time time off the homepage load.
The cause was eventually tracked down to calls out to TinyURL for
every node being rendered. The core problem came from the
<a href="http://drupal.org/project/service_links">service links</a> module. We
were able to fix it by disabling short URL&#8217;s in the module, but the
problem has been addressed in the current pre-release 2.x branch by
using caching. We might have ended up discovering this by disabling
module after module one at a time, but that would have taken
forever. In today&#8217;s world of API&#8217;s and social media, it&#8217;s very
common for a module to make calls to outside websites. However,
care should be taken by the module authors when coding in these
features. In our example, each call to TinyURL was fairly fast (300
- 400ms average), but on a homepage displaying 25 nodes, that adds
over 7 seconds page load time. Think of the impact if TinyURL
experienced a large slowdown, or even an outage? As far as I know,
Drupal doesn&#8217;t give you a way out of the box to track such
requests. However, using the tcpdump binary which is available on
virtually all Unix variants, we can see exactly what&#8217;s happening.
Note that you need root access to run tcpdump. Let&#8217;s say that the
IP address of your primary interface is 10.0.0.1. By using this
tcpdump command, we can see all outbound HTTP and HTTPS requests in
real time:</p>

<pre><code>tcpdump src host 10.0.0.1 and dst port 80 or dst port 443
</code></pre>

<p>If you don&#8217;t get any traffic after a few seconds, go hit your
/cron.php page - this should generate some traffic like this for
you to see:
Here we can see that our host is making a bunch of outbound
requests to master.drupal.org. This is because the &#8220;Update Status&#8221;
module is checking to see what upgrades are available for us. What
if you see traffic and don&#8217;t know what module is causing it? grep
to the rescue! In order to find out which module was making the
calls to TinyURL, we ran the following command:</p>

<pre><code>grep -R 'tinyurl.com' /path/to/drupal/sites/all/modules/\*
</code></pre>

<p>This returned one hit, from
/path/to/drupal/sites/all/modules/service_links/service_links.module.
By disabling the short links feature within the module we decreased
page load time by 7 seconds!</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Register for free DrupalCon tickets from DrupalModules.com!]]></title>
    <link href="http://sysadminsjourney.com/content/2010/02/25/register-free-drupalcon-tickets-drupalmodulescom"/>
    <updated>2010-02-25T00:00:00-06:00</updated>
    <id>http://sysadminsjourney.com/content/2010/02/25/register-for-free-drupalcon-tickets-from-drupalmodules-com</id>
    <content type="html"><![CDATA[<p>Make sure you sign to <a href="http://drupalmodules.com/articles/drupalcon-2010-ticket-contest">win free tickets to DrupalCon SF
2010</a> over at
<a href="http://www.drupalmodules.com">DrupalModules.com</a>.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Drupal StackScript for RH Derivatives on Linode (Instant Drupal!)]]></title>
    <link href="http://sysadminsjourney.com/content/2010/02/24/drupal-stackscript-rh-derivatives-linode-instant-drupal"/>
    <updated>2010-02-24T00:00:00-06:00</updated>
    <id>http://sysadminsjourney.com/content/2010/02/24/drupal-stackscript-for-rh-derivatives-on-linode-instant-drupal</id>
    <content type="html"><![CDATA[<p><a href="http://www.linode.com/stackscripts?r=c4f79463ba583ec1f15e330719%0A0bda4bda9d65df">StackScripts</a> are a relatively new offering from Linode that allow users to
build their own installation script by &#8220;stacking&#8221; previously existing scripts
together to build the machine you want. You can keep your <a href="http:/%0A/www.linode.com/stackscripts?r=c4f79463ba583ec1f15e3307190bda4bda9d65df">StackScript</a> to
yourself, or publish it for the world to use. Deploying a distribution with a
StackScript takes only about 5 minutes, afterwards you have a fully configured
system with applications up and running. Here&#8217;s a sneak-peek at a my <a href="http://www.linode.com/stackscripts/view/?Stack%0AScriptID=167&amp;r=c4f79463ba583ec1f15e3307190bda4bda9d65df">Drupal
StackScript for RH Derivatives</a> deployment just
before launch: <img src="http://sysadminsjourney.com/assets/images/stackscript-deploy.png" alt="" /> Many of the users of
<a href="http://www.linode.com/?r=c4f79463ba583ec1f15e3307190bda4bda9d65df">Linode</a>
appear to prefer Ubuntu, and there&#8217;s good reason &#8211; it&#8217;s a great distribution.
I run it on all my laptops and most of my desktops. However, I personally find
it a bit too bleeding edge for my servers and prefer CentOS for the server
room. Currently there are 16 StackScripts available for Ubuntu, with only 3
available for CentOS. Well, after today, there&#8217;s now 6 for CentOS! After
clicking the deploy button, you&#8217;ll have the images ready to go in less than a
minute. Boot the config, and the StackScript will run on first boot - when
it&#8217;s done, you&#8217;ll have a secured and configured LAMP stack,
<a href="http://drupal.org/project/drush">drush</a> install, <a href="http://drupal.org">Drupal</a>
install, and all updates applied via yum. I&#8217;ve published these StackScripts so
that anyone with a
<a href="http://www.linode.com/?r=c4f79463ba583ec1f15e3307190bda4bda9d65df">Linode</a>
can use them with their CentOS and Fedora installations:</p>

<ul>
<li><strong><a href="http://www.linode.com/stackscripts/view/?StackScriptID=154">StackScript Bash Library for RH Derivatives</a></strong>: This is a port of Chris Aker&#8217;s (of Linode) Bash Library for Ubuntu. You don&#8217;t use this script directly, it&#8217;s strictly for inclusion by other scripts.</li>
<li><strong><a href="http://www.linode.com/stackscripts/view/?StackScriptID=162">Drupal Library for RH Derivatives</a></strong>: This library provides two functions, install_drush and install_drupal.</li>
<li><strong><a href="http://www.linode.com/stackscripts/view/?StackScriptID=167">Drupal for RH Derivatives</a></strong>: This is a the StackScript used in the screenshot above. After clicking on the Deploy button, you&#8217;ll have a working Drupal installation up and running in about 5 minutes.
If you use them and find any bugs or have any RH-based StackScripts you&#8217;d like
made available, post a comment and I&#8217;ll see what I can do. In the interest of
full disclosure, the links to
<a href="http://www.linode.com/?r=c4f79463ba583ec1f15e3307190bda4bda9d65df">Linode</a> in
this article have a referral code in them that will give me $20 credit if you
sign up and keep your account open for 90 days. If you like these
StackScripts, please use my links to sign up for a Linode - you&#8217;ll wonder how
you did without one!</li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Sysadmin Humor]]></title>
    <link href="http://sysadminsjourney.com/content/2010/02/22/sysadmin-humor"/>
    <updated>2010-02-22T00:00:00-06:00</updated>
    <id>http://sysadminsjourney.com/content/2010/02/22/sysadmin-humor</id>
    <content type="html"><![CDATA[<p>I laughed out loud when I saw this <a href="http://xkcd.com">XKCD</a> comic this morning:
<img src="http://imgs.xkcd.com/comics/devotion_to_duty.png" alt="" /></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Performing a CentOS Anaconda-based Install on a Linode for Kickstart, Root LVM and SELinux Features]]></title>
    <link href="http://sysadminsjourney.com/content/2010/02/22/performing-centos-anaconda-based-install-linode-kickstart-root-lvm-and-selinux-features"/>
    <updated>2010-02-22T00:00:00-06:00</updated>
    <id>http://sysadminsjourney.com/content/2010/02/22/performing-a-centos-anaconda-based-install-on-a-linode-for-kickstart-root-lvm-and-selinux-features</id>
    <content type="html"><![CDATA[<p><a href="http://www.linode.com/?r=c4f79463ba583ec1f15e3307190bda4bda9d65df">Linode</a>
rocks. Seriously, read <a href="http://sysadminsjourney.com/content/2008/08/01/linode-review">my
review</a>. I was
talking to a co-worker (whom I converted to
<a href="http://www.linode.com/?r=c4f79463ba583ec1f15e3307190bda4bda9d65df">Linode</a> as
well) about how I would pay double the amount to keep my
<a href="http://www.linode.com/?r=c4f79463ba583ec1f15e3307190bda4bda9d65df">Linode</a>
now that I know how much I use it. Don&#8217;t tell them that, they&#8217;re cheap :) If
you find this article helpful (or my article about <a href="http://sysadminsjourney.com/content/2008/11/26/bringing-your-linode-%0Ahome-you">moving VM&#8217;s to and from
Linode</a>), please consider clicking one of the links in this article to sign
up for a
<a href="http://www.linode.com/?r=c4f79463ba583ec1f15e3307190bda4bda9d65df">Linode</a> -
if you sign up for 90 days, I&#8217;ll get $20 credited to my account. I was setting
up a second
<a href="http://www.linode.com/?r=c4f79463ba583ec1f15e3307190bda4bda9d65df">Linode</a>
that was to be a testing ground for some
<a href="http://blog.linode.com/2010/02/09/introducing-stackscripts/">StackScripts</a>
I&#8217;m working on. The new
<a href="http://www.linode.com/?r=c4f79463ba583ec1f15e3307190bda4bda9d65df">Linode</a>
will eventually replace my existing one. For whatever reason, the most recent
version of CentOS they had available was 5.3. Not a big deal, I can &#8216;yum
upgrade&#8217; up to 5.4 after installation. Well, after doing so, I found that a
lot of features that I wanted had been stripped out. In Linode&#8217;s defense, it&#8217;s
in their best interest to offer very stripped down images for their customers.
The one feature I wanted that I couldn&#8217;t get enabled was SELinux, and simply
installing the packages still wouldn&#8217;t let me use &#8216;setenforce 1&#8217; to get it
turned on. My best guess as to why is that the Linode kernel didn&#8217;t support
it, but I honestly didn&#8217;t troubleshoot it too much. I really wanted root LVM
capabilities as well, so I decided that a full-on anaconda based installation
was the way to go. Plus, I couldn&#8217;t find anything in the forums about it, so
there was the lure of being the first to do it ;-) Well, thanks to the
flexibility offered by
<a href="http://www.linode.com/?r=c4f79463ba583ec1f15e3307190bda4bda9d65df">Linode</a>,
not only can you do a anaconda-based installation (with optional Kickstart),
but you can do so using the GUI over VNC if you&#8217;re so inclined!</p>

<h3>Step 1: Setup Finnix</h3>

<p>Finnix is the distro of Linode&#8217;s choice for &#8216;rescue&#8217; operations on your
server. Think of it as a Swiss Army knife - it&#8217;s a very powerful tool that
takes very little setup. For more on Finnix, checkout the <a href="http://library.linode.com/troubleshooting/finnix-recovery">Linode Library
article</a>. First, we
need to setup a very small, 20MB ext3 disk that will house our installation
kernel and initrd. Set up another ext3 disk of 100MB to be mounted at /boot
for PV-GRUB. Finally, setup your raw disk that will be used for the OS
installation. Since we&#8217;ll be using LVM, you can easily add to and resize your
disk later, so don&#8217;t overdo it. I went with 5GB for my root disk. If you&#8217;re
following along, here&#8217;s what you should have: <img src="http://sysadminsjourney.com/assets/images/linode-%0Adisks.png" alt="" /> Now, setup a Finnix configuration profile. Click on &#8220;Create a new
configuration profile&#8221;, and type &#8220;Finnix Rescue&#8221; for the label. For the
kernel, select &#8220;Recovery - Finnix (kernel)&#8221;. For /dev/xvda, select the
&#8220;Recovery - Finnix (iso)&#8221;. For /dev/xvdb, select the &#8220;Centos 5.4 Install
Disk&#8221;. For uncompressed initrd image, select &#8220;Recovery - Finnix (initrd).
Leave the other settings at defaults, and save the profile. Here&#8217;s what it
should look like: <img src="http://sysadminsjourney.com/assets/images/linode-finnix.png" alt="" /></p>

<h3>Step 2: Upload the Xen-enabled Kernel and Initrd from the DVD</h3>

<p>Next, boot Finnix from your
<a href="http://www.linode.com/?r=c4f79463ba583ec1f15e3307190bda4bda9d65df">Linode</a>
control panel. Click on the console tab, and launch the AJAX console. Once at
the console, we need to mount our install disk, and fetch the xen-enabled
kernel and initrd from your favorite mirror. Mount the installation disk,
change directories, and download the files:</p>

<pre><code>mount /dev/xvdb /mnt/xvdb 
cd /mnt/xvdb
for f in initrd.img vmlinuz; do
wget http://mirror.unl.edu/centos/5.4/os/i386/images/xen/${f}
done
cd
umount /mnt/xvdb
</code></pre>

<p>Now, shutdown Finnix from the
<a href="http://www.linode.com/?r=c4f79463ba583ec1f15e3307190bda4bda9d65df">Linode</a>
control panel.</p>

<h3>Step 3: Setup the CentOS configuration profile</h3>

<p>Create a new profile, and name it CentOS 5.4. For the kernel, select &#8220;pv-grub-
x86_32&#8221;. For /dev/xvda, select &#8220;CentOS 5.4 PV-GRUB Boot&#8221;. For /dev/xvdb choose
&#8220;CentOS 5.4 OS Disk&#8221;. For /dev/xvdh, select our &#8220;CentOS 5.4 Install Disk&#8221;.
Point the root device to a custom location: &#8220;/dev/mapper/VolGroup00-LogVol00&#8221;.
Leave the rest as defaults, here&#8217;s a screenshot: <img src="http://sysadminsjourney.com/assets/images/linode-%0Acentos.png" alt="" /></p>

<h3>Step 4: Boot the CentOS configuration profile and start the installation</h3>

<p>Save the profile, and boot it. Note that it won&#8217;t boot automatically, we have
to point GRUB in the right direction first. You&#8217;ll be greeted by a scary-
looking &#8216;grubdom>&#8217; prompt. Now, we need to tell grub to boot our install
kernel and initrd:</p>

<pre><code>root (hd2)
kernel (hd2)/vmlinuz
initrd (hd2)/initrd.img
boot
</code></pre>

<p>Note that if you want to do a kickstart install, you would append
ks=http://my.com/this.ks to the kernel line above. More on this later. Once
the kernel loads, you&#8217;ll be presented with the familiar anaconda text-based
installer. Choose your language, and your installation type. I prefer HTTP
from a mirror. If you choose to do the same, use the mirror hostname for the
Web site name, and the path to the directory that contains all the release
notes &#8211; usually it&#8217;s /centos/5.4/os/i386/. Anaconda will fetch the stage2
image, then launch the installer. Here&#8217;s where it gets cool - it will give you
a choice to &#8220;Start VNC&#8221;. If you choose this option, you can connect to your
<a href="http://www.linode.com/?r=c4f79463ba583ec1f15e3307190bda4bda9d65df">Linode</a>
via VNC (note it launches on display 1, not 0), and complete the installation
via a GUI. Install as you would any other CentOS installation. <strong>Make note of
where your root directory is at.</strong> The installer may complain about your
/dev/xvdh being a loop device, tell anaconda to ignore it. Exclude /dev/xvda
from any partitioning, we&#8217;ll set that up manually later.</p>

<h3>Step 5: Modify the CentOS configuration profile and start the operating</h3>

<p>system</p>

<p>Once you click the &#8220;Reboot&#8221; button on the installer, you&#8217;ll be disconnected
from VNC. Your machine will be restarted, but it will stick at the grubdom
prompt again - that&#8217;s okay. We&#8217;ll be stuck at the grubdom> prompt one more
time - use this to tell it to boot CentOS using the boot partition the
installer setup for us:</p>

<pre><code>grubdom&gt; root (hd1,0)                                                          
grubdom&gt; kernel (hd1,0)/vmlinuz-2.6.18-164.el5xen                              
grubdom&gt; initrd (hd1,0)/initrd-2.6.18-164.el5xen.img                           
grubdom&gt; boot
</code></pre>

<p>You will then boot into CentOS - exit the system settings GUI - you can run it
again later by running system-config-securitylevel-tui. Now we need to setup
our boot disk so that pv-grub knows how to boot our kernel so we&#8217;re not
consistently prompted upon reboot.
<a href="http://www.linode.com/?r=c4f79463ba583ec1f15e3307190bda4bda9d65df">Linode</a>
uses PV-GRUB to boot our kernel, and it&#8217;s looking for a &#8216;boot&#8217; directory
directly on /dev/xvda. For more details, <a href="http://www.linode.com/wiki/index.php/PV-GRUB#Partitioning">see the Linode
Wiki</a>. Run this as
root, and make sure your block devices are aligned with mine before
copy/pasting:</p>

<pre><code>mkdir /mnt/newboot
mkfs.ext3 /dev/xvda
mount /dev/xvda /mnt/newboot
rsync -av /boot /mnt/newboot
cd /mnt/newboot/boot
ln -s . boot
cd
umount /mnt/newboot
umount /boot
e2label /dev/xvdb1 oldboot
e2label /dev/xvda /boot
</code></pre>

<p>That&#8217;s it - reboot, and you should be up and running! You can create a LVM
physical volume out of the old boot partition on /dev/xvdb1, or just leave it
around unused.</p>

<h3>Streamlining the process with Kickstart</h3>

<p>We can use kickstart to <strong>really</strong> streamline the process. Follow steps 1, 2,
and 3 above, but on step 4, replace the kernel line with this:</p>

<pre><code>kernel (hd2)/vmlinuz ks=http://www.sysadminsjourney.com/assets/files/linode-minimal.ks
</code></pre>

<p>That&#8217;s it! The kickstart file handles partitioning, and setting up the right
boot partition, as well as disabling unneeded services that you don&#8217;t need for
a Linode. Make sure you check out the file
http://www.sysadminsjourney.com/assets/files/linode-minimal.ks - your root password is there, change it immediately! Stay
tuned for my CentOS <a href="http://blog.linode.com/2010/02/09/introducing-stackscripts/">StackScripts</a>!</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Assign Different Values to Different Nodes via One Action in Views Bulk Operations]]></title>
    <link href="http://sysadminsjourney.com/content/2010/02/04/assign-different-values-different-nodes-one-action-views-bulk-operations"/>
    <updated>2010-02-04T00:00:00-06:00</updated>
    <id>http://sysadminsjourney.com/content/2010/02/04/assign-different-values-to-different-nodes-via-one-action-in-views-bulk-operations</id>
    <content type="html"><![CDATA[<p>The <a href="http://drupal.org/project/views_bulk_operations">Views Bulk
Operations</a> module
(a.k.a. VBO), is a godsend for busy <a href="http://drupal.org">Drupal</a> site
administrators. Don&#8217;t just take my word for it -
<a href="http://www.lullabot.com">Lullabot</a> wrote a chapter about it in
<a href="http://oreilly.com/catalog/9780596515805/">O&#8217;Reilly&#8217;s Using Drupal</a>,
it&#8217;s included in the <a href="http://openatrium.com/">Open Atrium Drupal
distribution</a>, and it&#8217;s even used on
<a href="http://drupal.org/node/520290">Drupal.org</a>! Out of the box, VBO does a
lot to streamline the things you do everyday, so that you spend less
time doing them. A perfect example is bulk content moderation - with a
few clicks of the mouse, you can mark a huge amount of comments as spam.
You can even enable batch processing with a single click of a mouse so
that you can literally do thousands of these without timing out.</p>

<p>VBO was
attractive enough that we decided to offload the bulk/batch operations
of <a href="http://drupal.org/project/node_gallery">Node Gallery</a> to VBO.
Integration for the most part was surprisingly easy - VBO &#8220;speaks&#8221; in
Drupal Actions, so by <a href="http://drupal.org/node/172152">writing actions</a>,
we were writing integration with VBO.</p>

<p>There&#8217;s one undocumented case
where VBO can be used that was critical for us. Most VBO actions you
will find perform one action to a set of nodes, one at a time. Often
times, that one action is to set a value of some sort on said nodes. In
the case of <a href="http://drupal.org/project/node_gallery">Node Gallery</a>, we
wanted to be able to assign different weight values (used for sorting)
to a bunch of nodes. The key here is that we aren&#8217;t assigning a value of
&#8216;2&#8217; to all selected node&#8217;s weight, we want to assign a weight of 2 to
node #1, 3 to node #2, 8 to node #3, and so on. While not
straightforward, it&#8217;s definitely achievable.</p>

<p>The general idea we&#8217;ll be
taking is to have VBO display a list of nodes to the admin. The admin
can place a checkmark next to the nodes that he wishes to change the
weight on, then select &#8220;Change the image&#8217;s weight&#8221; from the action
dropdown, and click submit. We will then draw a form that includes some
summary information about the nodes, and a select box with the node&#8217;s
current weight. The admin sets the weight he wants for each node, then
clicks submit. VBO then takes over, assigning each node the proper
weight. Let&#8217;s get into the code - first we implement
hook_action_info(), telling Drupal that we have actions to provide:</p>

<div>
  <pre><code class='php'>&lt;?php
/**
* Implementation of hook_action_info().
*/
function node_gallery_action_info() {
  return array(
    'node_gallery_change_image_weight_action' =&gt; array(
      'description' =&gt; t('Change image weight (sort order)'),
      'type' =&gt; 'node',
      'behavior' =&gt; array('changes_node_property'),
      'configurable' =&gt; TRUE,
      'hooks' =&gt; array(
        'node' =&gt; array('presave'),
      ),
    ),
  );
}</code></pre>
</div>


<p>The only real items of note in the hook above are setting &#8216;configurable&#8217;
to true, and setting &#8216;behavior&#8217; to &#8216;changes_node_property&#8217;. Setting
&#8216;configurable&#8217; allows us to display a custom form, and setting the
behavior tells VBO that we&#8217;ll be modifying the node. In turn, VBO will
call $node->save on each node after it&#8217;s been processed. Next, we
define our configurable action&#8217;s form function:</p>

<div>
  <pre><code class='php'>&lt;?php
function node_gallery_change_image_weight_action_form($context = array()) {
  //We're being called from VBO - we can do extra validation
  if ($context['view']-&gt;plugin_name == 'bulk') {
    //@todo: Add imagefield support in our sort form, and theme it with draggable items
    $sql = &quot;SELECT n.nid, n.title, ngi.weight FROM {node} n &quot; .
            &quot;INNER JOIN {node_gallery_images} ngi ON n.nid = ngi.nid &quot; .
            &quot;WHERE n.nid IN (&quot;. db_placeholders($context['selection']) .&quot;)&quot;;
    $result = db_query($sql,$context['selection']);
    $delta = count($context['selection']) &gt; 20 ? intval(count($context['selection'])/2) : 10;
    $form['node_gallery_change_image_weight_action']['#tree'] = TRUE;
    while ($image = db_fetch_object($result)) {
      $form['node_gallery_change_image_weight_action'][$image-&gt;nid]['title'] = array(
        '#type' =&gt; 'item',  
        '#value' =&gt; $image-&gt;title,
      );
      $form['node_gallery_change_image_weight_action'][$image-&gt;nid]['weight'] = array(
        '#type' =&gt; 'weight',
        '#title' =&gt; t('Weight'),
        '#default_value' =&gt; $image-&gt;weight,
        '#delta' =&gt; $delta,
      );
    }
  }
  //We're called from a standard advanced action where we assign one weight to all nodes
  else {
    $form['node_gallery_change_image_weight_action'] = array(
      '#type' =&gt; 'weight',
      '#title' =&gt; t('Weight'),
      '#description' =&gt; t('When listing images in a gallery, heavier items will sink at the lighter items will be positioned near the top'),
      '#delta' =&gt; 10,
    );
    if (isset($context['imageweight'])) {
      $form['node_gallery_change_image_weight_action']['#default_value'] = $context['imageweight'];
    }
  }
  return $form;
}</code></pre>
</div>


<p>To define your form function, simply append &#8216;_form&#8217; to your action name
and you have the function name. Nothing too wild and crazy in the form
function above, but there&#8217;s two key points:</p>

<ul>
<li>Line 3 shows you how you can detect when your function is being
called from a VBO view.</li>
<li>When your function is called from VBO, it will pass you the nid&#8217;s of
the selected nodes in the array $context[&#8216;selected&#8217;]</li>
</ul>


<p>Next, we define our submit function (you can define a validate function
if needed). Our submit function will pull the important data from the
submitted form and assemble it into a concise array that our action can
use. Here&#8217;s our submit function:</p>

<div>
  <pre><code class='php'>&lt;?php
function node_gallery_change_image_weight_action_submit($form, $form_state) {
  //We're setting all nodes to the same weight
  if (is_numeric($form_state['values']['node_gallery_change_image_weight_action'])) {
    $weight = $form_state['values']['node_gallery_change_image_weight_action']; 
  }
  //VBO is passing us a set of nids
  else {
    foreach ($form_state['values']['node_gallery_change_image_weight_action'] as $nid =&gt; $val) {
      $weight[$nid] = $val['weight'];
    }
  }
  return array('imageweight' =&gt; $weight);
}</code></pre>
</div>


<p>The key here is that if we are passed in the &#8220;single value&#8221; form, we
stick the value into the variable $weight as a simple scalar. If we are
passed in form data from the VBO &#8220;multi value&#8221; form, then $weight
becomes an associative array where the key is the nid, and the value is
the weight for that node.</p>

<p>Finally, we define our action function. Our
action is pretty simple, because it will only be called with one node
and one value. This is a key thing to remember when writing code for VBO
- even though you are working with batches of nodes, VBO is essentially
one big loop around the actions &#8211; it executes the action once for each
node. So, in our action, we simply check to see if the value of the
$context[&#8216;imageweight&#8217;] index that we passed from our submit function is
an integer or an array, and perform the correct operation on the node to
assign it it&#8217;s new weight. Once this function returns, VBO will call
$node->save for us.</p>

<div>
  <pre><code class='php'>&lt;?php
function node_gallery_change_image_weight_action(&amp;$node, $context = array()) {
  if (in_array($node-&gt;type, (array)node_gallery_get_types('image'))) {
    //All nodes are set to the same weight
    if (is_numeric($context['imageweight'])) {
      $node-&gt;weight = $context['imageweight'];
    }
    //VBO is sending us a list of nodes to modify with different weights
    else {
      $node-&gt;weight = $context['imageweight'][$node-&gt;nid];
    }
  } 
}</code></pre>
</div>


<p>While not always obvious, there&#8217;s not too many bulk operation conditions
that VBO can&#8217;t handle. Hats off to
<a href="http://drupal.org/user/48424">infojunkie</a> for writing such a helpful
module that is also easily integrated with!</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[HipHop PHP and Drupal]]></title>
    <link href="http://sysadminsjourney.com/content/2010/02/03/hiphop-php-and-drupal"/>
    <updated>2010-02-03T00:00:00-06:00</updated>
    <id>http://sysadminsjourney.com/content/2010/02/03/hiphop-php-and-drupal</id>
    <content type="html"><![CDATA[<p>So, <a href="http://www.facebook.com">Facebook</a> has <a href="http://developers.facebook.com/news.php?blog=1&amp;story=358">released HipHop
PHP</a> - a PHP-to-C++
converter. While the name is stupid, the idea is not. 100% of their developers
know PHP, I would guess that less than 5% of them are proficient at C++. So,
HipHop takes their PHP code, and converts it to compiled C++ &#8211; in turn, they
get a huge boost in performance and get to keep their existing developers.
HipHop is also it&#8217;s own webserver too - fun! My first thought was: I wonder
what this could mean for Drupal? Well, <a href="http://fourkitchens.com/authors/david-timothy-strauss">David
Struass</a>, a maintainer
of <a href="http://fourkitchens.com/pressflow-makes-drupal-scale">Pressflow (a set of patches for Drupal performance and
scalability)</a> put up a
<a href="http://fourkitchens.com/blog/2010/02/03/making-drupal-pressflow-%0Amore-mundane">blog post</a> about what it would take for Pressflow and Drupal to become
HipHop-friendly. Exciting times!</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Apache mod_proxy '[error] (13)Permission denied' error on RHEL]]></title>
    <link href="http://sysadminsjourney.com/content/2010/02/01/apache-modproxy-error-13permission-denied-error-rhel"/>
    <updated>2010-02-01T00:00:00-06:00</updated>
    <id>http://sysadminsjourney.com/content/2010/02/01/apache-mod_proxy-error-13permission-denied-error-on-rhel</id>
    <content type="html"><![CDATA[<p>Had an interesting issue today working on a mod_proxy setup of Apache
forwarding requests in a reverse proxy setup to a backend Tomcat server. No
matter what I did, I kept getting this in Apache&#8217;s error log:</p>

<pre><code>[error] (13)Permission denied: proxy: AJP: attempt to connect to 10.x.x.x:7009 (virtualhost.virtualdomain.com) failed
</code></pre>

<p>I thought for sure it was proxy permissions, but nothing I did fixed the
issue. Then it hit me: SELinux! Why I always think of SELinux last when it&#8217;s
responsible for 90% of my problems, I&#8217;ll never know. SELinux on RHEL/CentOS by
default ships so that httpd processes cannot initiate outbound connections,
which is just what mod_proxy attempts to do. If this is your problem, you&#8217;ll
see something like this in /var/log/audit/audit.log:</p>

<pre><code>type=AVC msg=audit(1265039669.305:14): avc:  denied  { name_connect } for  pid=4343 comm="httpd" dest=7009 
scontext=system_u:system_r:httpd_t:s0 tcontext=system_u:object_r:port_t:s0 tclass=tcp_socket
</code></pre>

<p>To fix this, first test by setting the boolean dynamically (not permanent
yet):</p>

<pre><code> /usr/sbin/setsebool httpd_can_network_connect 1
</code></pre>

<p>If that works, you can set it so that the default policy is changed and this
setting will persist across reboots:</p>

<pre><code> /usr/sbin/setsebool -P httpd_can_network_connect 1
</code></pre>

<p>Hope this saves others some time!</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Teaching Java How to Commit Suicide]]></title>
    <link href="http://sysadminsjourney.com/content/2010/01/29/teaching-java-how-commit-suicide"/>
    <updated>2010-01-29T00:00:00-06:00</updated>
    <id>http://sysadminsjourney.com/content/2010/01/29/teaching-java-how-to-commit-suicide</id>
    <content type="html"><![CDATA[<p>At $work, we have a lot of java processes that are ran via cron and other
wrappers that do some pretty critical tasks. The apps have been written so
that the whole thing is wrapped in a try/catch that will call system.exit(1)
should something not go right. Our wrapper scripts watch for a non-zero exit
code, and alert Nagios if something went wrong. This works great except for
when a VM encounters an outOfMemory exception (OOM). The Java VM attempts to
continue on, but if the main thread hits this exception, the entire VM will
exit. However, the application code that exits with a status of 1 never gets
called, so the application ends up dying with a status of 0. Well, Sun (Oracle
now I guess) gave us a new option in Java 6 that was backported to 1.4.2_12
and up that allows us to tell Java to run a shell command when it encounters
an OOM exception. By adding the option</p>

<pre><code>-XX:OnOutOfMemoryError="kill -9 %p"
</code></pre>

<p>to our Java command line, the VM will execute a shell that calls the kill
command, with an argument of the PID of the VM. The -9 option to kill will
cause the VM to exit with a non-zero status, so that our wrappers will pick up
the error and alert the right people. Note: this feature was never backported
to Java5 - sorry!</p>
]]></content>
  </entry>
  
</feed>
