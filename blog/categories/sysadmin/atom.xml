<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: sysadmin | SysAdmin's Journey]]></title>
  <link href="http://sysadminsjourney.com/blog/categories/sysadmin/atom.xml" rel="self"/>
  <link href="http://sysadminsjourney.com/"/>
  <updated>2012-09-18T18:32:11-05:00</updated>
  <id>http://sysadminsjourney.com/</id>
  <author>
    <name><![CDATA[Justin Ellison]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Using Git Submodules with Dynamic Puppet Environments]]></title>
    <link href="http://sysadminsjourney.com/content/using-git-submodules-dynamic-puppet-environments"/>
    <updated>2012-02-06T00:00:00-06:00</updated>
    <id>http://sysadminsjourney.com/content/using-git-submodules-with-dynamic-puppet-environments</id>
    <content type="html"><![CDATA[<p>There comes a point in the lifecycle of every Puppet setup where you realize
that you're going to be much better off utilizing other peoples' Puppet
modules whenever possible.  It's what makes OSS great -- why should I reinvent
the wheel when I can help make your wheel even better?  I've found what I
think is a very productive setup -- it leverages Git (specifically branches,
submodules, and hooks), Gitolite permissions, and Puppet environments to
create a workflow that a team of admins can use to iterate over new features
on without disturbing each other.</p>

<p>Most of pieces to this puzzle are very well documented elsewhere, I'll provide
links where necessary.</p>

<h2>Step 1: Establish Dynamic Environment Workflow</h2>

<p>The first step is to go read "<a href="http://puppetlabs.com/blog/git-workflow-and-puppet-environments/">Git Workflow and Puppet
Environments</a>" written by Adrien Thebo of Puppet Labs.  Once you've
implemented that setup, you should be able to do the following from your
workstation:
<div>
  <pre><code class='bash'>git clone git@git:puppet.git
cd puppet
git checkout -b mybrokenbranch
echo &quot;this line breaks everything&quot; &gt;&gt; manifests/site.pp
git commit -am 'Intentionally breaking things'
git push origin mybrokenbranch</code></pre>
</div>
</p>

<p>At this point, you now have a new environment named 'mybrokenbranch' on your
Puppetmaster. You can test the setup by ssh'ing into the client machines and
run:
<div>
  <pre><code class='bash'>puppet agent --test --environment mybrokenbranch --noop</code></pre>
</div>
</p>

<p>That obviously won't be a happy puppet run. The key
point here being that your other environments are not impacted by the work of
this one admin. Let's delete the local and remote branch. From your
workstation:
<div>
  <pre><code class='bash'>git checkout master
git branch -d mybrokenbranch
git push origin :mybrokenbranch</code></pre>
</div>
</p>

<p>Note that
the Puppetmaster says that it's deleted the environment. Feel free to verify
that by running the above command on the Puppet client, it will complain about
not having an environment.</p>

<h2>Step 2: Incorporate Git Submodules</h2>

<p>With all that setup, let's go ahead and implement support for git submodules.
I have a pull request off to Adrien to implement this functionality, but until
he commits it in, you can use <a href="https://github.com/justintime/puppet-git-hooks">my fork on github</a>. Replace the update
hook with the updated version on your git server. Now, let's try pulling a git
submodule into our repo. Again, from your workstation:
<div>
  <pre><code class='bash'>git checkout -b firewall
git submodule add git://github.com/puppetlabs/puppetlabs-firewall.git modules/firewall
git add .gitmodules modules/firewall
git commit -m 'Adding firewall submodule'
git push origin firewall</code></pre>
</div>
</p>

<p>Note in the output that the Puppetmaster is checking out the
git submodule into the new environment. Go ahead and log into the
Puppetmaster, and look in your firewall environment, you should see all the
manifests and whatnot there.</p>

<p>Here's where I need to stamp a disclosure notice -- git submodules aren't all
milk and honey. There's some funky situations you can get yourself into if
you're not careful. Thankfully, there's not many of those situations you can't
get yourself out of. I highly recommend reading the <a href="http://progit.org/book/ch6-6.html">Pro Git chapter on submodules</a> before doing anything with
them.</p>

<h2>Step 3: Implement Access Controls on Gitolite</h2>

<p>This next step is entirely optional, but works out well for us. We have a
setup where I'm the only admin that can write to the master and testing
branches of our git repo, but any sysadmin can create their own branch, test
it, and delete it if need be. <a href="http://sitaramc.github.com/gitolite/">Setting up gitolite</a> is far beyond the scope of
this post, but if you have about an hour of free time, you can have it setup
and running. However, below I've pasted the relevant snippet from
gitolite.conf that enforces those permissions.
<div>
  <pre><code class='bash'>repo    puppet                                                                                                        &lt;br/&gt;
  RW+     = JustinEllison
  R       = @SysAdmins Fisheye-puppet PuppetMaster&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;           - master testing = @SysAdmins
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;  RW+     = @SysAdmins</code></pre>
</div>
</p>

<h2>Step 4: Profit!</h2>

<p>To summarize it all, here's the workflow for an admin to add a new feature in
our Puppet setup:</p>

<ol>
<li>Create a new VM which will be the testing ground for the new feature.</li>
<li>Create a local feature branch to implement the new feature in. The admin iterates over this branch (pushing the branch to origin) getting things working with his VM.</li>
<li>Once he's happy with the results on his VM, he's required to login to another sandbox VM, and run it against the same puppet branch with the '--noop' flag to ensure nothing unintended happens.</li>
<li>At this point, the positive and the negative have been tested, and he then asks me to merge the feature branch into master.</li>
<li>I then do a <div>
  <pre><code class='bash'>git diff ...origin/newfeature</code></pre>
</div>
 We go over any changes, and I merge it in.</li>
<li>From there, we follow our normal deployment method of tagging a release, and manually checking out the tag on the Puppetmaster.</li>
</ol>


<p>While it's certainly not perfect, this workflow setup has allowed us to work
together as a team while still implementing some best practices. In
particular, the dynamic environments allow us to test our features extensively
before releasing them into production. This is especially important in a team
where the admins aren't Ruby programmers that can write puppet-rspec tests.</p>

<p>Before the integration of git submodules with the dynamic environment
workflow, we were manually merging external repos into our own setup, and it
was an absolute nightmare. Now, to update our repo to use a new version of
someone else's module, we just create a new feature branch, update the
submodule, test, and merge.</p>

<p>What workflows do you and your team use that make life with Puppet better?
Please share below.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[VPS.net review]]></title>
    <link href="http://sysadminsjourney.com/content/2011/11/14/vpsnet-review"/>
    <updated>2011-11-14T00:00:00-06:00</updated>
    <id>http://sysadminsjourney.com/content/2011/11/14/vps-net-review</id>
    <content type="html"><![CDATA[<p>I've been running a single node from <a href="http://vps.net">VPS.net</a> for about a
year now.  Please note that my specific experience has been in their "Chicago
Zone D data center", but if you check out their <a href="http://status.vps.net">status
page</a> or search
<a href="http://twitter.com/#!/search/%23vpsnet">Twitter</a>, you'll find a lot of others
having the same issues.  While there's a lot of good things to write about,
where they fail is the most important area to me: availabilty. The pros of
using VPS.net include pricing, control panel, and console level access.  As is
typical for a VPS provider, they give you many "add-on" options such as
backup, etc that you can enable -- I've not investigated them myself.  Perhaps
the one of the nicest features is the ability to add server resources or
"nodes" on the fly with minimal downtime. However, it seems that VPS.net has
made a horrible choice in selecting what SAN they use to back their VM's.
Examine the graphic below: <img src="/assets/images/vps-net-availabiltiy.png" alt="" /> As you
can see, I'm getting less than 2 nines worth of uptime from my node.  Each and
everytime there's been an issue, support has been quick to point out that
they've had some sort of SAN issue, and that the SAN is 'resyncing'.  The
problem is that while the SAN is resyncing, I/O to my node is so horrible, I
can't cat a 500 byte file to stdout in less than 10 seconds.  So, the node
will respond to a ping, but it can't serve up a static image via Apache.  For
all intents and purposes, that's down in my book. The <a href="http://status.vps.net/2011/10/chi-d-cloud/">last SAN
synchronization</a> took the better
part of two days, during which time my node was unusable. In my experience,
the SAN is the most important building block when architecting a service
that's meant to be highly available.  Until VPS.net can address their SAN
issues, they are likely to continue to have prolonged downtimes.  Until that's
been fixed, there's just no way I can recommend their services to anyone.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Lead SysAdmin position available]]></title>
    <link href="http://sysadminsjourney.com/content/2011/04/06/lead-sysadmin-position-available"/>
    <updated>2011-04-06T00:00:00-05:00</updated>
    <id>http://sysadminsjourney.com/content/2011/04/06/lead-sysadmin-position-available</id>
    <content type="html"><![CDATA[<p>There's a blog post to follow with when/why, etc., but without
further ado: I'm moving to a new position at Buckle, and that means
we need a new Lead SysAdmin.  It's a great job at a great company,
in a great place to raise a family (Kearney, NE).  You get paid
well, get a good yearly budget for new toys, and equipment, and
it's overall a very fun position. If interested, <a href="http://sysadminsjourney.com/contact">drop me a
line</a>, and I'll make sure your
resume gets the proper attention.  To apply online, <a href="https://storefront.kenexa.com/buckle/cc/CCJobSearchAction.ss?command=CCSearchPage">click
here</a>,
and search for jobs within 5 miles of zip code 68845.  The job
title is "Web Development - Lead Systems Administrator". Here's the
job posting: JOB DETAIL Job Title:  ** Web Development - Lead
Systems Administrator**
<img src="https://storefront.kenexa.com/buckle/cc/images/orange_line.gif" alt="image" />
Location:  Buckle Corporate Office &amp; Distribution Center  2407 W
24th Street  KEARNEY, Nebraska 68845-0000
<img src="https://storefront.kenexa.com/buckle/cc/images/orange_line.gif" alt="image" />
Job Description:  ### Lead Systems Administrator **Position
Summary:** The Lead Systems Administrator will be responsible for
the deployment and maintenance of Unix/Linux systems and
application software in multiple environments. The ideal candidate
will possess a deep understanding of large scale Unix deployments
and will lead the team responsible for the infrastructure serving
all e-commerce and intranet applications. Additionally, this person
must be able to function effectively in a fast-paced environment
where projects range from maintenance to upgrades to new
deployments and technologies. Our Systems Administrators also serve
as Network Administrators for the smaller networks their systems
reside in, so strong knowledge of ethernet, TCP/IP, and network
security is required. **Responsibilities:** • Maintain all
servers and workstations on WSD team, including production,
development, and staging of servers for the e-commerce platform and
company intranet • Setup, maintain, and manage an enterprise-class
backup strategy for WSD team servers and workstations • Automate
tasks via custom scripting • Assist in architecting and designing
solid server solutions **Requirements:** • Expertise in setting
up robust and reliable server architectures. Additionally, a large
appetite for automating the mundane is preferred and will be
encouraged. ● In-depth knowledge of technologies that include but
are not limited to: Linux systems administration, Java VM tuning,
Weblogic Administration, Apache HTTPD/NGINX Administration, All
layers of TCP/IP, subnetting, etc., IPSEC VPN’s, ISC BIND, Load
balancing and clustering technologies, Shell scripting, Nagios
monitoring and RRD administration, RPM packaging format and
patching best practices • A bachelor’s degree in Computer Science
or other discipline • Minimum 4-5 years of previous
system-administration experience in a professional environment
**Compensation:** Market/negotiable, relocation assistance is a
possibility for the right candidate.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Sysadmin Humor]]></title>
    <link href="http://sysadminsjourney.com/content/2010/02/22/sysadmin-humor"/>
    <updated>2010-02-22T00:00:00-06:00</updated>
    <id>http://sysadminsjourney.com/content/2010/02/22/sysadmin-humor</id>
    <content type="html"><![CDATA[<p>I laughed out loud when I saw this <a href="http://xkcd.com">XKCD</a> comic this morning:
<img src="http://imgs.xkcd.com/comics/devotion_to_duty.png" alt="" /></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Apache mod_proxy '[error] (13)Permission denied' error on RHEL]]></title>
    <link href="http://sysadminsjourney.com/content/2010/02/01/apache-modproxy-error-13permission-denied-error-rhel"/>
    <updated>2010-02-01T00:00:00-06:00</updated>
    <id>http://sysadminsjourney.com/content/2010/02/01/apache-mod_proxy-error-13permission-denied-error-on-rhel</id>
    <content type="html"><![CDATA[<p>Had an interesting issue today working on a mod_proxy setup of Apache
forwarding requests in a reverse proxy setup to a backend Tomcat server. No
matter what I did, I kept getting this in Apache's error log:</p>

<pre><code>[error] (13)Permission denied: proxy: AJP: attempt to connect to 10.x.x.x:7009 (virtualhost.virtualdomain.com) failed
</code></pre>

<p>I thought for sure it was proxy permissions, but nothing I did fixed the
issue. Then it hit me: SELinux! Why I always think of SELinux last when it's
responsible for 90% of my problems, I'll never know. SELinux on RHEL/CentOS by
default ships so that httpd processes cannot initiate outbound connections,
which is just what mod_proxy attempts to do. If this is your problem, you'll
see something like this in /var/log/audit/audit.log:</p>

<pre><code>type=AVC msg=audit(1265039669.305:14): avc:  denied  { name_connect } for  pid=4343 comm="httpd" dest=7009 
scontext=system_u:system_r:httpd_t:s0 tcontext=system_u:object_r:port_t:s0 tclass=tcp_socket
</code></pre>

<p>To fix this, first test by setting the boolean dynamically (not permanent
yet):</p>

<pre><code> /usr/sbin/setsebool httpd_can_network_connect 1
</code></pre>

<p>If that works, you can set it so that the default policy is changed and this
setting will persist across reboots:</p>

<pre><code> /usr/sbin/setsebool -P httpd_can_network_connect 1
</code></pre>

<p>Hope this saves others some time!</p>
]]></content>
  </entry>
  
</feed>
